{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import utils\n",
    "import encoder as enc\n",
    "import dataset as ds\n",
    "import train as tr\n",
    "import decoder as dec\n",
    "import dataset\n",
    "import modeltrainer as mt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#======= FLAGS ==========\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "tf.app.flags.DEFINE_integer('maxout_layer_size', 40, 'Maxout layer size')\n",
    "tf.app.flags.DEFINE_integer('max_sequence_length', 160, 'Max length of context')\n",
    "tf.app.flags.DEFINE_integer('max_question_length', 40, 'Max question tokens length')\n",
    "tf.app.flags.DEFINE_float('learning_rate', 0.0005, 'Learning Rate')\n",
    "tf.app.flags.DEFINE_integer('maxout_pooling_size', 8, 'Maxout pooling size')\n",
    "tf.app.flags.DEFINE_integer('lstm_size', 40, 'LSTM cell internal size')\n",
    "tf.app.flags.DEFINE_string('log_path', '/tmp/working/logs', 'logs location')\n",
    "tf.app.flags.DEFINE_integer('acc_batch_size', 10, 'How many examples to use to calculate accuracy')\n",
    "#tf.app.flags.DEFINE_integer('train_batch_size', 20, 'Train Batch Size')\n",
    "tf.app.flags.DEFINE_integer('max_decoder_iterations', 4, 'Decoder Iterations')\n",
    "tf.app.flags.DEFINE_integer('max_epoch', 100, 'Max Train Epoch Count')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove all variables\n",
    "#tf.reset_default_graph();\n",
    "\n",
    "lstm_size = FLAGS.lstm_size\n",
    "acc_batch_size = FLAGS.acc_batch_size\n",
    "word_vector_size = 300\n",
    "maxout_pooling_size = FLAGS.maxout_pooling_size\n",
    "max_decoder_iterations = FLAGS.max_decoder_iterations\n",
    "maxout_layer_size = FLAGS.maxout_layer_size;\n",
    "max_epoch = FLAGS.max_epoch;\n",
    "max_sequence_length = FLAGS.max_sequence_length;\n",
    "max_question_length = FLAGS.max_question_length\n",
    "\n",
    "\n",
    "batch_size = tf.placeholder(tf.int32, ())\n",
    "learning_rate_ph = tf.placeholder(tf.float32, ())\n",
    "\n",
    "\n",
    "dropout_rate_ph = tf.placeholder(tf.float32)\n",
    "question_ph = tf.placeholder(tf.float32, [None, max_question_length, word_vector_size], name=\"q_input\")\n",
    "document_ph = tf.placeholder(tf.float32, [None, max_sequence_length, word_vector_size], name=\"d_input\")\n",
    "doc_len_ph = tf.placeholder(tf.int32, [None])\n",
    "que_len_ph = tf.placeholder(tf.int32, [None])\n",
    "start_true = tf.placeholder(tf.int32, [None]);\n",
    "end_true   = tf.placeholder(tf.int32, [None]);\n",
    "document_size = doc_len_ph\n",
    "question_size = que_len_ph\n",
    "\n",
    "with tf.name_scope('ENCODER'):\n",
    "    # LSTM cell initialization\n",
    "    lstm = tf.nn.rnn_cell.LSTMCell(lstm_size)\n",
    "    lstm = tf.nn.rnn_cell.DropoutWrapper(cell=lstm, output_keep_prob=dropout_rate_ph)\n",
    "\n",
    "\n",
    "# LSTM cells for Bi-LSTM for COATINATION ENCODER\n",
    "with tf.name_scope('COATTENTION_ENCODER'):\n",
    "    lstm_cenc_fw = tf.nn.rnn_cell.LSTMCell(lstm_size)\n",
    "    lstm_cenc_fw = tf.nn.rnn_cell.DropoutWrapper(cell=lstm_cenc_fw, output_keep_prob=dropout_rate_ph)\n",
    "    lstm_cenc_bw = tf.nn.rnn_cell.LSTMCell(lstm_size)\n",
    "    lstm_cenc_bw = tf.nn.rnn_cell.DropoutWrapper(cell=lstm_cenc_bw, output_keep_prob=dropout_rate_ph)\n",
    "\n",
    "# create lstm cell for DYNAMIC POINTING DECODER\n",
    "lstm_dec = tf.nn.rnn_cell.LSTMCell(lstm_size)\n",
    "# get lstm initial state of zeroes\n",
    "#lstm_dec_state = lstm_dec.zero_state(1, tf.float32)\n",
    "#start_pos = 0; # ?generate random between (0, document_size-1)\n",
    "#end_pos = 0;   # ?generate random between (0, document_size-1)\n",
    "\n",
    "# create sentinel vector variable for both encodings \n",
    "#with tf.variable_scope(\"scope1\") as scope:\n",
    "sentinel_q = tf.get_variable(\"sentinel_q\", [ 1, lstm_size ], initializer = tf.random_normal_initializer())\n",
    "sentinel_d = tf.get_variable(\"sentinel_d\", [ 1, lstm_size ], initializer = tf.random_normal_initializer()) \n",
    "\n",
    "tf.summary.histogram('sentinel_q', sentinel_q)\n",
    "tf.summary.histogram('sentinel_q_max', tf.reduce_max(sentinel_q))\n",
    "tf.summary.histogram('sentinel_d', sentinel_d)\n",
    "tf.summary.histogram('sentinel_d_max', tf.reduce_max(sentinel_d))\n",
    "\n",
    "# optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate_ph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (batch, D, 2L)\n",
    "U = enc.encoderBatch(\n",
    "    document_ph, question_ph, \n",
    "    document_size, question_size, \n",
    "    lstm, lstm_cenc_fw, lstm_cenc_bw, \n",
    "    sentinel_d, sentinel_q,\n",
    "    batch_size,\n",
    "    FLAGS)\n",
    "#print(U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== DYNAMIC POINTING DECODER =============\n",
    "\n",
    "iter_start_scores, iter_end_scores = dec.decoderBatch(U, lstm_dec, dropout_rate_ph, batch_size, doc_len_ph, FLAGS)\n",
    "sum_start_scores = tf.reduce_sum(iter_start_scores, 2)\n",
    "sum_end_scores = tf.reduce_sum(iter_end_scores, 2)\n",
    "\n",
    "sum_loss, accuracy, pr_start_idx, pr_end_idx = tr.loss_and_accuracy(start_true, end_true, batch_size, sum_start_scores, sum_end_scores, max_sequence_length)\n",
    "# sum_loss, accuracy, pr_start_idx, pr_end_idx = tr.loss_and_accuracy_v2(\n",
    "#     start_true, end_true, batch_size, \n",
    "#     iter_start_scores, iter_end_scores, \n",
    "#     max_sequence_length, FLAGS.max_decoder_iterations)\n",
    "\n",
    "#tf.summary.histogram('sum_start_scores', sum_start_scores)\n",
    "#tf.summary.histogram('sum_end_scores', sum_end_scores)\n",
    "#tf.summary.scalar('loss_test', sum_loss, [\"TEST_STAT\"])\n",
    "tf.summary.scalar('loss_train', sum_loss)\n",
    "#tf.summary.scalar('avg_accuracy_test',  accuracy_avg, [\"TEST_STAT\"])\n",
    "#tf.summary.scalar('avg_accuracy_train', accuracy_avg, [\"TRAIN_STAT\"])\n",
    "\n",
    "\n",
    "with tf.name_scope('Train'):\n",
    "    train_step = optimizer.minimize(sum_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#=========== Training ==================\n",
    "\n",
    "# summary_op_train = tf.summary.merge_all(\"TRAIN_STAT\")\n",
    "# summary_op_test = tf.summary.merge_all(\"TEST_STAT\")\n",
    "# summary_op = tf.summary.merge_all()\n",
    "\n",
    "# embeddings = dataset.Embeddings('./english/glove.840B.300d.w2vformat.bin')\n",
    "# #embeddings = dataset.Embeddings('./processed_ruscorpora_1_300_10.bin')\n",
    "# init = tf.global_variables_initializer()\n",
    "# DATASET_LENGTH = 20\n",
    "# BATCH_SIZE = 5\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(init)\n",
    "#     h_param_str = utils.make_h_param_string(\n",
    "#         FLAGS.learning_rate, \n",
    "#         FLAGS.lstm_size, \n",
    "#         max_sequence_length, \n",
    "#         FLAGS.maxout_pooling_size,\n",
    "#         DATASET_LENGTH,\n",
    "#         BATCH_SIZE\n",
    "#     )\n",
    "#     writer = tf.summary.FileWriter(FLAGS.log_path + \"/6-\" + h_param_str, sess.graph)\n",
    "#     with open('./english/test_160.csv') as file_test:\n",
    "#         for epoch_ in range(100 or max_epoch):\n",
    "#             step_ = 0;\n",
    "#             with open('./english/train_160.csv') as file_train:\n",
    "#                 while True:\n",
    "#                     feed_dict = tr.processLineBatch(file_train, embeddings, BATCH_SIZE, \n",
    "#                                                     max_sequence_length, max_question_length, \n",
    "#                                                     question_ph, document_ph, dropout_rate_ph,\n",
    "#                                                     doc_len_ph, que_len_ph, start_true, end_true,\n",
    "#                                                     batch_size,\n",
    "#                                                     0.5)\n",
    "#                     if feed_dict is None: break\n",
    "#                     tr.trainStep(sess, \n",
    "#                                  feed_dict, \n",
    "#                                  writer, \n",
    "#                                  train_step, sum_loss, accuracy_avg, summary_op, summary_op_train, \n",
    "#                                  epoch_ * DATASET_LENGTH + step_, profiling=False)\n",
    "#                     step_+= 1\n",
    "#                     if (step_ >= DATASET_LENGTH): break;\n",
    "                        \n",
    "#             print('Epoch', epoch_, 'completed')\n",
    "#             test_params = tr.processLineBatch(file_test, embeddings, BATCH_SIZE, \n",
    "#                                               max_sequence_length, max_question_length, \n",
    "#                                               question_ph, document_ph, dropout_rate_ph,\n",
    "#                                               doc_len_ph, que_len_ph, start_true, end_true,\n",
    "#                                               batch_size,\n",
    "#                                               1)\n",
    "#             tr.accuracyTest(sess, test_params, writer, \n",
    "#                             accuracy_avg, summary_op, summary_op_test, pr_start_idx, pr_end_idx, epoch_)\n",
    "#         print('End')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings are loaded to memory\n",
      "dropout =  0.7 ; batch_size =  10 ; lrate= 0.001\n",
      "dropout =  0.7 ; batch_size =  10 ; lrate= 0.001\n",
      "dropout =  0.7 ; batch_size =  10 ; lrate= 0.001\n",
      "dropout =  0.7 ; batch_size =  10 ; lrate= 0.001\n",
      "dropout =  0.7 ; batch_size =  10 ; lrate= 0.001\n",
      "dropout =  0.7 ; batch_size =  10 ; lrate= 0.001\n",
      "dropout =  0.7 ; batch_size =  10 ; lrate= 0.001\n",
      "dropout =  0.7 ; batch_size =  10 ; lrate= 0.001\n",
      "dropout =  0.7 ; batch_size =  10 ; lrate= 0.001\n",
      "dropout =  0.7 ; batch_size =  10 ; lrate= 0.001\n",
      "dropout =  0.7 ; batch_size =  10 ; lrate= 0.001\n",
      "dropout =  0.7 ; batch_size =  10 ; lrate= 0.001\n",
      "dropout =  0.7 ; batch_size =  10 ; lrate= 0.001\n",
      "dropout =  0.7 ; batch_size =  10 ; lrate= 0.001\n",
      "dropout =  0.7 ; batch_size =  10 ; lrate= 0.001\n",
      "dropout =  0.7 ; batch_size =  10 ; lrate= 0.001\n",
      "dropout =  0.7 ; batch_size =  10 ; lrate= 0.001\n",
      "dropout =  0.7 ; batch_size =  10 ; lrate= 0.001\n",
      "dropout =  0.7 ; batch_size =  10 ; lrate= 0.001\n",
      "dropout =  0.7 ; batch_size =  10 ; lrate= 0.001\n",
      "dropout =  0.7 ; batch_size =  10 ; lrate= 0.001\n",
      "dropout =  0.7 ; batch_size =  10 ; lrate= 0.001\n",
      "dropout =  0.7 ; batch_size =  10 ; lrate= 0.001\n",
      "dropout =  0.7 ; batch_size =  10 ; lrate= 0.001\n",
      "dropout =  0.7 ; batch_size =  10 ; lrate= 0.001\n"
     ]
    }
   ],
   "source": [
    "variables = {\n",
    "    'max_sequence_length': max_sequence_length,\n",
    "    'max_question_length': max_question_length,\n",
    "    'question_ph': question_ph,\n",
    "    'document_ph': document_ph,\n",
    "    'dropout_rate_ph': dropout_rate_ph,\n",
    "    'doc_len_ph': doc_len_ph,\n",
    "    'que_len_ph': que_len_ph,\n",
    "    'start_true_ph': start_true,\n",
    "    'end_true_ph': end_true,\n",
    "    'batch_size_ph': batch_size,\n",
    "    'learning_rate_ph': learning_rate_ph\n",
    "}\n",
    "\n",
    "ops = {\n",
    "    'train_step_op': train_step, \n",
    "    'sum_loss_op': sum_loss, \n",
    "    'accuracy_op': accuracy,\n",
    "    'pr_start_idx_op': pr_start_idx, \n",
    "    'pr_end_idx_op': pr_end_idx\n",
    "}\n",
    "\n",
    "hps = mt.HyperParamsSpace(dropouts = [0.7], batches = [10], lrates=[0.001])\n",
    "with mt.ModelTrainer('./english/glove.840B.300d.w2vformat.bin', FLAGS.log_path, 'old_loss') as trainer:\n",
    "    trainer.set_variables(variables)\n",
    "    trainer.set_ops(ops)\n",
    "    for i_ in range(0, 1):\n",
    "        hp = hps.getRand()\n",
    "        trainer.reset(hp)\n",
    "        step = 0;\n",
    "        for epoch_ in range(0, 100):\n",
    "            \n",
    "            h_param_str = utils.make_h_param_string_2(hp)\n",
    "            step = trainer.train(hp, './english/train_160.csv', 500, step);\n",
    "            trainer.accuracy(hp, './english/train_160.csv', epoch_, 500, 'train', 20);\n",
    "            trainer.accuracy(hp, './english/test_160.csv', epoch_, 500, 'valid', 20);\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
