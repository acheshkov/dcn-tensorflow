{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import utils\n",
    "import encoder as enc\n",
    "import dataset as ds\n",
    "import train as tr\n",
    "import decoder as dec\n",
    "import dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ArgumentError",
     "evalue": "argument --maxout_layer_size: conflicting option string: --maxout_layer_size",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArgumentError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-0695bcffa97c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#======= FLAGS ==========\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mFLAGS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFINE_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'maxout_layer_size'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Maxout layer size'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFINE_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'max_sequence_length'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m160\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Max length of context'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFINE_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'max_question_length'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Max question tokens length'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/platform/flags.py\u001b[0m in \u001b[0;36mDEFINE_integer\u001b[0;34m(flag_name, default_value, docstring)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mdocstring\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m \u001b[0mhelpful\u001b[0m \u001b[0mmessage\u001b[0m \u001b[0mexplaining\u001b[0m \u001b[0mthe\u001b[0m \u001b[0muse\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m   \"\"\"\n\u001b[0;32m---> 96\u001b[0;31m   \u001b[0m_define_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflag_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/platform/flags.py\u001b[0m in \u001b[0;36m_define_helper\u001b[0;34m(flag_name, default_value, docstring, flagtype)\u001b[0m\n\u001b[1;32m     68\u001b[0m                               \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                               \u001b[0mhelp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdocstring\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m                               type=flagtype)\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/argparse.py\u001b[0m in \u001b[0;36madd_argument\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1346\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"length of metavar tuple does not match nargs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0madd_argument_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/argparse.py\u001b[0m in \u001b[0;36m_add_action\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m   1709\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_add_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption_strings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1711\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optionals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1712\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1713\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_positionals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/argparse.py\u001b[0m in \u001b[0;36m_add_action\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m   1550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_add_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1552\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ArgumentGroup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1553\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_group_actions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/argparse.py\u001b[0m in \u001b[0;36m_add_action\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m   1360\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_add_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \u001b[0;31m# resolve any conflicts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1362\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_conflict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m         \u001b[0;31m# add to actions list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/argparse.py\u001b[0m in \u001b[0;36m_check_conflict\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m   1499\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfl_optionals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m             \u001b[0mconflict_handler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0mconflict_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfl_optionals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_handle_conflict_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconflicting_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/argparse.py\u001b[0m in \u001b[0;36m_handle_conflict_error\u001b[0;34m(self, action, conflicting_actions)\u001b[0m\n\u001b[1;32m   1508\u001b[0m                                      \u001b[0;32mfor\u001b[0m \u001b[0moption_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m                                      in conflicting_actions])\n\u001b[0;32m-> 1510\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mArgumentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mconflict_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1512\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_handle_conflict_resolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconflicting_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mArgumentError\u001b[0m: argument --maxout_layer_size: conflicting option string: --maxout_layer_size"
     ]
    }
   ],
   "source": [
    "#======= FLAGS ==========\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "tf.app.flags.DEFINE_integer('maxout_layer_size', 100, 'Maxout layer size')\n",
    "tf.app.flags.DEFINE_integer('max_sequence_length', 160, 'Max length of context')\n",
    "tf.app.flags.DEFINE_integer('max_question_length', 40, 'Max question tokens length')\n",
    "tf.app.flags.DEFINE_float('learning_rate', 0.005, 'Learning Rate')\n",
    "tf.app.flags.DEFINE_integer('maxout_pooling_size', 8, 'Maxout pooling size')\n",
    "tf.app.flags.DEFINE_integer('lstm_size', 100, 'LSTM cell internal size')\n",
    "tf.app.flags.DEFINE_string('log_path', '/tmp/working/logs', 'logs location')\n",
    "tf.app.flags.DEFINE_integer('acc_batch_size', 10, 'How many examples to use to calculate accuracy')\n",
    "#tf.app.flags.DEFINE_integer('train_batch_size', 20, 'Train Batch Size')\n",
    "tf.app.flags.DEFINE_integer('max_decoder_iterations', 4, 'Decoder Iterations')\n",
    "tf.app.flags.DEFINE_integer('max_epoch', 100, 'Max Train Epoch Count')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove all variables\n",
    "#tf.reset_default_graph();\n",
    "\n",
    "lstm_size = FLAGS.lstm_size\n",
    "acc_batch_size = FLAGS.acc_batch_size\n",
    "word_vector_size = 300\n",
    "maxout_pooling_size = FLAGS.maxout_pooling_size\n",
    "max_decoder_iterations = FLAGS.max_decoder_iterations\n",
    "maxout_layer_size = FLAGS.maxout_layer_size;\n",
    "max_epoch = FLAGS.max_epoch;\n",
    "max_sequence_length = FLAGS.max_sequence_length;\n",
    "max_question_length = FLAGS.max_question_length\n",
    "\n",
    "\n",
    "batch_size = tf.placeholder(tf.int32, ())\n",
    "\n",
    "\n",
    "dropout_rate_ph = tf.placeholder(tf.float32)\n",
    "question_ph = tf.placeholder(tf.float32, [None, max_question_length, word_vector_size], name=\"q_input\")\n",
    "document_ph = tf.placeholder(tf.float32, [None, max_sequence_length, word_vector_size], name=\"d_input\")\n",
    "doc_len_ph = tf.placeholder(tf.int32, [None])\n",
    "que_len_ph = tf.placeholder(tf.int32, [None])\n",
    "start_true = tf.placeholder(tf.int32, [None]);\n",
    "end_true   = tf.placeholder(tf.int32, [None]);\n",
    "document_size = doc_len_ph\n",
    "question_size = que_len_ph\n",
    "\n",
    "with tf.name_scope('ENCODER'):\n",
    "    # LSTM cell initialization\n",
    "    lstm = tf.nn.rnn_cell.LSTMCell(lstm_size)\n",
    "    lstm = tf.nn.rnn_cell.DropoutWrapper(cell=lstm, output_keep_prob=dropout_rate_ph)\n",
    "\n",
    "\n",
    "# LSTM cells for Bi-LSTM for COATINATION ENCODER\n",
    "with tf.name_scope('COATTENTION_ENCODER'):\n",
    "    lstm_cenc_fw = tf.nn.rnn_cell.LSTMCell(lstm_size)\n",
    "    lstm_cenc_fw = tf.nn.rnn_cell.DropoutWrapper(cell=lstm_cenc_fw, output_keep_prob=dropout_rate_ph)\n",
    "    lstm_cenc_bw = tf.nn.rnn_cell.LSTMCell(lstm_size)\n",
    "    lstm_cenc_bw = tf.nn.rnn_cell.DropoutWrapper(cell=lstm_cenc_bw, output_keep_prob=dropout_rate_ph)\n",
    "\n",
    "# create lstm cell for DYNAMIC POINTING DECODER\n",
    "lstm_dec = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "# get lstm initial state of zeroes\n",
    "#lstm_dec_state = lstm_dec.zero_state(1, tf.float32)\n",
    "#start_pos = 0; # ?generate random between (0, document_size-1)\n",
    "#end_pos = 0;   # ?generate random between (0, document_size-1)\n",
    "\n",
    "# create sentinel vector variable for both encodings \n",
    "#with tf.variable_scope(\"scope1\") as scope:\n",
    "sentinel_q = tf.get_variable(\"sentinel_q\", [ 1, lstm_size ], initializer = tf.random_normal_initializer())\n",
    "sentinel_d = tf.get_variable(\"sentinel_d\", [ 1, lstm_size ], initializer = tf.random_normal_initializer()) \n",
    "\n",
    "tf.summary.histogram('sentinel_q', sentinel_q)\n",
    "tf.summary.histogram('sentinel_q_max', tf.reduce_max(sentinel_q))\n",
    "tf.summary.histogram('sentinel_d', sentinel_d)\n",
    "tf.summary.histogram('sentinel_d_max', tf.reduce_max(sentinel_d))\n",
    "\n",
    "# optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=FLAGS.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"COATTENTION_ENCODER_1/Slice_2:0\", shape=(?, ?, ?), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# (batch, D, 2L)\n",
    "U = enc.encoderBatch(\n",
    "    document_ph, question_ph, \n",
    "    document_size, question_size, \n",
    "    lstm, lstm_cenc_fw, lstm_cenc_bw, \n",
    "    sentinel_d, sentinel_q,\n",
    "    batch_size,\n",
    "    FLAGS)\n",
    "print(U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ===================== DYNAMIC POINTING DECODER =============\n",
    "\n",
    "sum_start_scores, sum_end_scores = dec.decoderBatch(U, lstm_dec, dropout_rate_ph, batch_size, FLAGS)\n",
    "sum_loss, accuracy_avg, pr_start_idx, pr_end_idx = tr.loss_and_accuracy(start_true, end_true, batch_size, sum_start_scores, sum_end_scores, max_sequence_length)\n",
    "\n",
    "tf.summary.histogram('sum_start_scores', sum_start_scores)\n",
    "tf.summary.histogram('sum_end_scores', sum_end_scores)\n",
    "tf.summary.scalar('loss_test', sum_loss, [\"TEST_STAT\"])\n",
    "tf.summary.scalar('loss_train', sum_loss, [\"TRAIN_STAT\"])\n",
    "tf.summary.scalar('avg_accuracy_test',  accuracy_avg, [\"TEST_STAT\"])\n",
    "tf.summary.scalar('avg_accuracy_train', accuracy_avg, [\"TRAIN_STAT\"])\n",
    "\n",
    "\n",
    "with tf.name_scope('Train'):\n",
    "    train_step = optimizer.minimize(sum_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings are loaded to memory\n",
      "Epoch 0 completed\n",
      "AVG accuracy 0.0\n",
      "Epoch 1 completed\n",
      "AVG accuracy 0.1\n",
      "Epoch 2 completed\n",
      "AVG accuracy 0.0\n",
      "Epoch 3 completed\n",
      "AVG accuracy 0.2\n",
      "Epoch 4 completed\n",
      "AVG accuracy 0.0\n",
      "Epoch 5 completed\n",
      "AVG accuracy 0.0410256410256\n",
      "Epoch 6 completed\n",
      "AVG accuracy 0.0\n",
      "Epoch 7 completed\n",
      "AVG accuracy 0.0\n",
      "Epoch 8 completed\n",
      "AVG accuracy 0.0\n",
      "Epoch 9 completed\n",
      "AVG accuracy 0.0\n",
      "Epoch 10 completed\n",
      "AVG accuracy 0.0\n",
      "Epoch 11 completed\n",
      "AVG accuracy 0.2\n",
      "Epoch 12 completed\n",
      "AVG accuracy 0.0\n",
      "Epoch 13 completed\n",
      "AVG accuracy 0.2\n",
      "Epoch 14 completed\n",
      "AVG accuracy 0.2\n",
      "Epoch 15 completed\n",
      "AVG accuracy 0.0\n",
      "Epoch 16 completed\n",
      "AVG accuracy 0.0\n",
      "Epoch 17 completed\n",
      "AVG accuracy 0.0\n",
      "Epoch 18 completed\n",
      "AVG accuracy 0.0\n",
      "Epoch 19 completed\n",
      "AVG accuracy 0.0\n",
      "Epoch 20 completed\n",
      "AVG accuracy 0.333333333333\n",
      "Epoch 21 completed\n",
      "AVG accuracy 0.2\n",
      "Epoch 22 completed\n",
      "AVG accuracy 0.18\n",
      "Epoch 23 completed\n",
      "AVG accuracy 0.0\n",
      "Epoch 24 completed\n",
      "AVG accuracy 0.0\n",
      "Epoch 25 completed\n",
      "AVG accuracy 0.1\n",
      "Epoch 26 completed\n",
      "AVG accuracy 0.3\n",
      "Epoch 27 completed\n",
      "AVG accuracy 0.0\n",
      "Epoch 28 completed\n",
      "AVG accuracy 0.2\n",
      "Epoch 29 completed\n",
      "AVG accuracy 0.0\n",
      "Epoch 30 completed\n",
      "AVG accuracy 0.0\n",
      "Epoch 31 completed\n",
      "AVG accuracy 0.2\n",
      "Epoch 32 completed\n",
      "AVG accuracy 0.4\n",
      "Epoch 33 completed\n",
      "AVG accuracy 0.0\n",
      "Epoch 34 completed\n",
      "AVG accuracy 0.0\n",
      "Epoch 35 completed\n",
      "AVG accuracy 0.0\n",
      "Epoch 36 completed\n",
      "AVG accuracy 0.0\n",
      "Epoch 37 completed\n",
      "AVG accuracy 0.0\n",
      "Epoch 38 completed\n",
      "AVG accuracy 0.2\n",
      "Epoch 39 completed\n",
      "AVG accuracy 0.2\n",
      "Epoch 40 completed\n",
      "AVG accuracy 0.133333333333\n",
      "Epoch 41 completed\n",
      "AVG accuracy 0.0\n",
      "Epoch 42 completed\n",
      "AVG accuracy 0.2\n",
      "Epoch 43 completed\n",
      "AVG accuracy 0.0\n",
      "Epoch 44 completed\n",
      "AVG accuracy 0.2\n",
      "Epoch 45 completed\n",
      "AVG accuracy 0.0\n",
      "Epoch 46 completed\n",
      "AVG accuracy 0.2\n",
      "Epoch 47 completed\n",
      "AVG accuracy 0.2\n",
      "Epoch 48 completed\n",
      "AVG accuracy 0.0\n",
      "Epoch 49 completed\n",
      "AVG accuracy 0.0\n",
      "Epoch 50 completed\n",
      "AVG accuracy 0.0\n",
      "Epoch 51 completed\n",
      "AVG accuracy 0.0\n",
      "Epoch 52 completed\n",
      "AVG accuracy 0.2\n",
      "Epoch 53 completed\n",
      "AVG accuracy 0.0\n",
      "Epoch 54 completed\n",
      "AVG accuracy 0.0186046511628\n",
      "Epoch 55 completed\n",
      "AVG accuracy 0.0\n",
      "Epoch 56 completed\n",
      "AVG accuracy 0.0\n",
      "Epoch 57 completed\n",
      "AVG accuracy 0.347878787879\n",
      "Epoch 58 completed\n",
      "AVG accuracy 0.1\n",
      "Epoch 59 completed\n",
      "AVG accuracy 0.0\n",
      "Epoch 60 completed\n",
      "AVG accuracy 0.0\n",
      "Epoch 61 completed\n",
      "AVG accuracy 0.0\n",
      "Epoch 62 completed\n",
      "AVG accuracy 0.0\n",
      "Epoch 63 completed\n",
      "AVG accuracy 0.0\n",
      "Epoch 64 completed\n",
      "AVG accuracy 0.0\n",
      "Epoch 65 completed\n",
      "AVG accuracy 0.0\n",
      "Epoch 66 completed\n",
      "AVG accuracy 0.1\n",
      "Epoch 67 completed\n",
      "AVG accuracy 0.2\n",
      "Epoch 68 completed\n",
      "AVG accuracy 0.1\n",
      "Epoch 69 completed\n",
      "AVG accuracy 0.0\n",
      "Epoch 70 completed\n",
      "AVG accuracy 0.0\n",
      "Epoch 71 completed\n",
      "AVG accuracy 0.0\n",
      "Epoch 72 completed\n",
      "AVG accuracy 0.0\n",
      "Epoch 73 completed\n",
      "AVG accuracy 0.0\n",
      "Epoch 74 completed\n",
      "AVG accuracy 0.133333333333\n",
      "Epoch 75 completed\n",
      "AVG accuracy 0.0\n",
      "Epoch 76 completed\n",
      "AVG accuracy 0.0\n",
      "Epoch 77 completed\n",
      "AVG accuracy 0.08\n",
      "Epoch 78 completed\n",
      "AVG accuracy 0.0\n",
      "Epoch 79 completed\n",
      "AVG accuracy 0.1\n",
      "Epoch 80 completed\n",
      "AVG accuracy 0.0\n",
      "Epoch 81 completed\n",
      "AVG accuracy 0.0\n",
      "Epoch 82 completed\n",
      "AVG accuracy 0.0\n",
      "Epoch 83 completed\n",
      "AVG accuracy 0.0\n",
      "Epoch 84 completed\n",
      "AVG accuracy 0.2\n",
      "Epoch 85 completed\n",
      "AVG accuracy 0.0\n",
      "Epoch 86 completed\n",
      "AVG accuracy 0.0\n",
      "Epoch 87 completed\n",
      "AVG accuracy 0.133333333333\n",
      "Epoch 88 completed\n",
      "AVG accuracy 0.2\n",
      "Epoch 89 completed\n",
      "AVG accuracy 0.1\n",
      "Epoch 90 completed\n",
      "AVG accuracy 0.0\n",
      "Epoch 91 completed\n",
      "AVG accuracy 0.0\n",
      "Epoch 92 completed\n",
      "AVG accuracy 0.0\n",
      "Epoch 93 completed\n",
      "AVG accuracy 0.0\n",
      "Epoch 94 completed\n",
      "AVG accuracy 0.1\n",
      "Epoch 95 completed\n",
      "AVG accuracy 0.0444444444444\n",
      "Epoch 96 completed\n",
      "AVG accuracy 0.4\n",
      "Epoch 97 completed\n",
      "AVG accuracy 0.0\n",
      "Epoch 98 completed\n",
      "AVG accuracy 0.0\n",
      "Epoch 99 completed\n",
      "AVG accuracy 0.133333333333\n",
      "End\n"
     ]
    }
   ],
   "source": [
    "#=========== Training ==================\n",
    "\n",
    "summary_op_train = tf.summary.merge_all(\"TRAIN_STAT\")\n",
    "summary_op_test = tf.summary.merge_all(\"TEST_STAT\")\n",
    "summary_op = tf.summary.merge_all()\n",
    "\n",
    "embeddings = dataset.Embeddings('./english/glove.840B.300d.w2vformat.bin')\n",
    "#embeddings = dataset.Embeddings('./processed_ruscorpora_1_300_10.bin')\n",
    "init = tf.global_variables_initializer()\n",
    "DATASET_LENGTH = 20\n",
    "BATCH_SIZE = 5\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    h_param_str = utils.make_h_param_string(FLAGS.learning_rate, FLAGS.lstm_size, max_sequence_length, FLAGS.maxout_pooling_size)\n",
    "    writer = tf.summary.FileWriter(FLAGS.log_path + \"/1-\" + h_param_str, sess.graph)\n",
    "    with open('./english/test_160.csv') as file_test:\n",
    "        for epoch_ in range(100 or max_epoch):\n",
    "            step_ = 0;\n",
    "            with open('./english/train_160.csv') as file_train:\n",
    "                while True:\n",
    "                    feed_dict = tr.processLineBatch(file_train, embeddings, BATCH_SIZE, \n",
    "                                                    max_sequence_length, max_question_length, \n",
    "                                                    question_ph, document_ph, dropout_rate_ph,\n",
    "                                                    doc_len_ph, que_len_ph, start_true, end_true,\n",
    "                                                    batch_size,\n",
    "                                                    0.5)\n",
    "                    if feed_dict is None: break\n",
    "                    tr.trainStep(sess, \n",
    "                                 feed_dict, \n",
    "                                 writer, \n",
    "                                 train_step, sum_loss, accuracy_avg, summary_op, summary_op_train, \n",
    "                                 epoch_ * DATASET_LENGTH + step_, profiling=False)\n",
    "                    step_+= 1\n",
    "                    if (step_ >= DATASET_LENGTH): break;\n",
    "                        \n",
    "            print('Epoch', epoch_, 'completed')\n",
    "            test_params = tr.processLineBatch(file_test, embeddings, BATCH_SIZE, \n",
    "                                              max_sequence_length, max_question_length, \n",
    "                                              question_ph, document_ph, dropout_rate_ph,\n",
    "                                              doc_len_ph, que_len_ph, start_true, end_true,\n",
    "                                              batch_size,\n",
    "                                              1)\n",
    "            tr.accuracyTest(sess, test_params, writer, \n",
    "                            accuracy_avg, summary_op, summary_op_test, pr_start_idx, pr_end_idx, epoch_)\n",
    "        print('End')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
