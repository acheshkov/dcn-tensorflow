{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "#import maxout\n",
    "import highway_maxout as hmn\n",
    "import utils\n",
    "import encoder as enc\n",
    "import dataset as ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#======= FLAGS ==========\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "tf.app.flags.DEFINE_integer('maxout_layer_size', 20, 'Maxout layer size')\n",
    "tf.app.flags.DEFINE_integer('max_sequence_length', 160, 'Max length of context')\n",
    "tf.app.flags.DEFINE_integer('max_question_length', 40, 'Max question tokens length')\n",
    "tf.app.flags.DEFINE_float('learning_rate', 0.001, 'Learning Rate')\n",
    "tf.app.flags.DEFINE_integer('maxout_pooling_size', 8, 'Maxout pooling size')\n",
    "tf.app.flags.DEFINE_integer('lstm_size', 20, 'LSTM cell internal size')\n",
    "tf.app.flags.DEFINE_string('log_path', '/tmp/working/logs', 'logs location')\n",
    "tf.app.flags.DEFINE_integer('acc_batch_size', 5, 'How many examples to use to calculate accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove all variables\n",
    "#tf.reset_default_graph();\n",
    "\n",
    "lstm_size = FLAGS.lstm_size\n",
    "acc_batch_size = FLAGS.acc_batch_size\n",
    "word_vector_size = 300\n",
    "maxout_pooling_size = FLAGS.maxout_pooling_size\n",
    "max_decoder_iterations = 4\n",
    "maxout_layer_size = FLAGS.maxout_layer_size;\n",
    "max_epoch = 100;\n",
    "max_sequence_length = FLAGS.max_sequence_length;\n",
    "max_question_length = FLAGS.max_question_length\n",
    "\n",
    "\n",
    "dropout_rate_ph = tf.placeholder(tf.float32)\n",
    "question_ph = tf.placeholder(tf.float32, [1, max_question_length, word_vector_size], name=\"q_input\")\n",
    "document_ph = tf.placeholder(tf.float32, [1, max_sequence_length, word_vector_size], name=\"d_input\")\n",
    "doc_len_ph = tf.placeholder(tf.int32, ())\n",
    "que_len_ph = tf.placeholder(tf.int32, ())\n",
    "document_size = doc_len_ph\n",
    "question_size = que_len_ph\n",
    "\n",
    "with tf.name_scope('ENCODER'):\n",
    "    # LSTM cell initialization\n",
    "    lstm = tf.nn.rnn_cell.LSTMCell(lstm_size)\n",
    "    lstm = tf.nn.rnn_cell.DropoutWrapper(cell=lstm, output_keep_prob=dropout_rate_ph)\n",
    "\n",
    "\n",
    "# LSTM cells for Bi-LSTM for COATINATION ENCODER\n",
    "with tf.name_scope('COATTENTION_ENCODER'):\n",
    "    lstm_cenc_fw = tf.nn.rnn_cell.LSTMCell(lstm_size)\n",
    "    lstm_cenc_fw = tf.nn.rnn_cell.DropoutWrapper(cell=lstm_cenc_fw, output_keep_prob=dropout_rate_ph)\n",
    "    lstm_cenc_bw = tf.nn.rnn_cell.LSTMCell(lstm_size)\n",
    "    lstm_cenc_bw = tf.nn.rnn_cell.DropoutWrapper(cell=lstm_cenc_bw, output_keep_prob=dropout_rate_ph)\n",
    "\n",
    "# create lstm cell for DYNAMIC POINTING DECODER\n",
    "lstm_dec = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "# get lstm initial state of zeroes\n",
    "#lstm_dec_state = lstm_dec.zero_state(1, tf.float32)\n",
    "start_pos = 0; # ?generate random between (0, document_size-1)\n",
    "end_pos = 0;   # ?generate random between (0, document_size-1)\n",
    "\n",
    "# create sentinel vector variable for both encodings \n",
    "#with tf.variable_scope(\"scope1\") as scope:\n",
    "sentinel_q = tf.get_variable(\"sentinel_q\", [ lstm_size , 1], initializer = tf.random_normal_initializer())\n",
    "sentinel_d = tf.get_variable(\"sentinel_d\", [ lstm_size , 1], initializer = tf.random_normal_initializer()) \n",
    "\n",
    "#tf.summary.histogram('sentinel_q', sentinel_q)\n",
    "#tf.summary.histogram('sentinel_q_max', tf.reduce_max(sentinel_q))\n",
    "#tf.summary.histogram('sentinel_d', sentinel_d)\n",
    "#tf.summary.histogram('sentinel_d_max', tf.reduce_max(sentinel_d))\n",
    "\n",
    "# optimizer\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=FLAGS.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Slice_1:0\", shape=(?, ?), dtype=float32)\n",
      "Tensor(\"COATTENTION_ENCODER_1/strided_slice:0\", shape=(?, 40), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "U = enc.encoder(\n",
    "    document_ph, question_ph, \n",
    "    document_size, question_size, \n",
    "    lstm, lstm_cenc_fw, lstm_cenc_bw, \n",
    "    sentinel_d, sentinel_q, \n",
    "    FLAGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ===================== DYNAMIC POINTING DECODER =============\n",
    "\n",
    "# returns tuple (scores_start, scores_end, strart_pos, start_end, new_lstm_state)\n",
    "def decoderIteration(U, lstm_state, start_pos, end_pos, iter_number):\n",
    "    with tf.name_scope('Decoder_Iteration'):\n",
    "        with tf.name_scope('Next_Start'):\n",
    "            scores_start = hmn.HMN2(U, \n",
    "                               tf.transpose(lstm_state.h), \n",
    "                               tf.slice(U, [0, start_pos], [lstm_size*2, 1]) ,\n",
    "                               tf.slice(U, [0, end_pos], [lstm_size*2, 1]) , \n",
    "                               document_size,\n",
    "                               'start',\n",
    "                                FLAGS,\n",
    "                                dropout_rate_ph,\n",
    "                                iter_number)\n",
    "\n",
    "            new_start_pos = tf.to_int32(tf.argmax(scores_start, 0))\n",
    "\n",
    "        with tf.name_scope('Next_End'):\n",
    "            scores_end = hmn.HMN2(U, \n",
    "                             tf.transpose(lstm_state.h), \n",
    "                             tf.slice(U, [0, new_start_pos], [lstm_size*2, 1],) ,\n",
    "                             tf.slice(U, [0, end_pos], [lstm_size*2, 1]), \n",
    "                             document_size,\n",
    "                            'end',\n",
    "                            FLAGS,\n",
    "                            dropout_rate_ph,\n",
    "                            iter_number)\n",
    "            new_end_pos = tf.to_int32(tf.argmax(scores_end, 0))\n",
    "        \n",
    "        with tf.name_scope('LSTM_State_Update'):\n",
    "            lstm_input = tf.concat(\n",
    "                [tf.slice(U, [0, new_start_pos], [lstm_size*2, 1], name='slice-5'), tf.slice(U, [0, new_end_pos], [lstm_size*2, 1])],\n",
    "                axis = 0\n",
    "            )\n",
    "            output, new_lstm_state = lstm_dec(tf.reshape(lstm_input, [1, lstm_size*4]), lstm_state)\n",
    "        \n",
    "        return scores_start, scores_end, new_start_pos , new_end_pos, new_lstm_state\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with tf.name_scope('DYNAMIC_POINTING_DECODER'):\n",
    "    i = tf.constant(0)\n",
    "    prev_start_pos = tf.constant(-1, shape=tf.TensorShape([]), dtype=tf.int32)\n",
    "    prev_end_pos = tf.constant(-1, shape=tf.TensorShape([]), dtype=tf.int32)\n",
    "    start_pos = tf.constant(0, shape=tf.TensorShape([]), dtype=tf.int32)\n",
    "    end_pos = tf.constant(0, shape=tf.TensorShape([]), dtype=tf.int32)\n",
    "    #prev_start_pos = 0;\n",
    "    sum_start_scores = tf.zeros([1, document_size])\n",
    "    sum_end_scores = tf.zeros([1, document_size])\n",
    "    lstm_dec_state = lstm_dec.zero_state(1, tf.float32)\n",
    "    while_data = (i, (prev_start_pos, prev_end_pos), (start_pos, end_pos), (sum_start_scores, sum_end_scores), lstm_dec_state, U)\n",
    "    while_cond = lambda i, prev, cur, sum, st, u: tf.logical_and(tf.less_equal(i, max_decoder_iterations), tf.logical_or(tf.not_equal(prev[0], cur[0]), tf.not_equal(prev[1], cur[1])))\n",
    "\n",
    "    def while_body(i, prev, cur, summ, state, U):\n",
    "        scores_start, scores_end, new_start_pos, new_end_pos, lstm_dec_state = decoderIteration(U, state, cur[0], cur[1], tf.add(i, 1))\n",
    "        scores_start.set_shape([1, None])\n",
    "        scores_end.set_shape([1, None])\n",
    "        new_start_pos.set_shape(tf.TensorShape([]))\n",
    "        new_end_pos.set_shape(tf.TensorShape([]))\n",
    "        sum_start_scores = tf.add(summ[0], scores_start)\n",
    "        sum_end_scores   = tf.add(summ[1], scores_end)\n",
    "        return (tf.add(i, 1), cur, (new_start_pos, new_end_pos), (sum_start_scores, sum_end_scores), lstm_dec_state, U)\n",
    "    \n",
    "    _, _, _, (sum_start_scores, sum_end_scores), _, _ = tf.while_loop(while_cond, while_body, while_data)\n",
    "\n",
    "\n",
    "start_end_true = tf.placeholder(tf.int32, [2]);\n",
    "    \n",
    "#sum_start_scores = tf.Print(sum_start_scores, [start_end_true, sum_start_scores], 'sum_start_scores: ')\n",
    "#sum_end_scores = tf.Print(sum_end_scores, [start_end_true, sum_end_scores], 'sum_start_scores: ')\n",
    "    \n",
    "# loss and train step\n",
    "\n",
    "\n",
    "#end_true = tf.placeholder(tf.int32, ());\n",
    "onehot_labels = tf.one_hot(start_end_true, document_size)\n",
    "with tf.name_scope('Loss'):\n",
    "    sum_loss = tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels = onehot_labels,\n",
    "        logits = tf.concat([sum_start_scores, sum_end_scores], axis=0))\n",
    "    sum_loss = tf.reduce_max(sum_loss)\n",
    "\n",
    "\n",
    "tf.summary.histogram('sum_start_scores', sum_start_scores)\n",
    "tf.summary.histogram('sum_end_scores', sum_end_scores)\n",
    "tf.summary.scalar('loss_test', sum_loss, [\"TEST_STAT\"])\n",
    "tf.summary.scalar('loss', sum_loss, [\"TRAIN_STAT\"])\n",
    "    \n",
    "with tf.name_scope('Accuracy'):\n",
    "    with tf.name_scope('Prediction'):\n",
    "        pr_start_idx = tf.to_int32(tf.argmax(sum_start_scores, 1))[0]\n",
    "        pr_end_idx = tf.to_int32(tf.argmax(sum_end_scores, 1))[0]\n",
    "        \n",
    "    with tf.name_scope('Accuracy'):\n",
    "        accuracy = tf.py_func(utils.f1_score_int, [pr_start_idx, pr_end_idx, start_end_true[0], start_end_true[1]], tf.float64)\n",
    "        accuracy = tf.Print(accuracy, [start_end_true, [pr_start_idx, pr_end_idx], accuracy], 'True and Predicted: ')\n",
    "#tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "avg_accuracy_ph = tf.placeholder(tf.float32, ())\n",
    "tf.summary.scalar('avg_accuracy', avg_accuracy_ph, [\"TEST_STAT\"])\n",
    "tf.summary.scalar('avg_accuracy_train', accuracy, [\"TRAIN_STAT\"])\n",
    "\n",
    "\n",
    "with tf.name_scope('Train'):\n",
    "    train_step = optimizer.minimize(sum_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 completed\n"
     ]
    }
   ],
   "source": [
    "#=========== Training ==================\n",
    "\n",
    "last_avg_accuracy = 0.0\n",
    "\n",
    "def processLine(str, max_doc_length, max_que_length):\n",
    "    start_pos, end_pos, doc, que = str.split(';')\n",
    "    start_pos = int(start_pos)\n",
    "    end_pos = int(end_pos)\n",
    "    document = doc.split(' ')\n",
    "    question = que.split(' ')\n",
    "    doc_v = ds.sentence2Vectors_onstring(document, max_doc_length)\n",
    "    que_v = ds.sentence2Vectors_onstring(question, max_que_length)\n",
    "    return start_pos, end_pos, document, question, doc_v, que_v\n",
    "\n",
    "\n",
    "def accuracyValidation(acc_batch_size, step):\n",
    "    global last_avg_accuracy\n",
    "    acc_accum = 0\n",
    "    summary_op_test = tf.summary.merge_all(\"TEST_STAT\")\n",
    "    for step_accuracy_ in range(acc_batch_size):\n",
    "        start_true, end_true, doc, que, doc_v, que_v = sess.run(next_element_valid)\n",
    "        \n",
    "        acc, stat, stat_test, s, e = sess.run(\n",
    "            (accuracy, summary_op, summary_op_test, pr_start_idx, pr_end_idx),\n",
    "            feed_dict={\n",
    "                question_ph: [que_v], \n",
    "                document_ph: [doc_v], \n",
    "                start_end_true: [start_true, end_true],\n",
    "                doc_len_ph: len(doc),\n",
    "                que_len_ph: len(que),\n",
    "                dropout_rate_ph: 1.0,\n",
    "                avg_accuracy_ph: last_avg_accuracy\n",
    "            }\n",
    "        )\n",
    "        #print('Predicted answer', utils.substr(doc, s, e))\n",
    "        #print('True answer', utils.substr(doc, start_true, end_true))\n",
    "        #writer.add_summary(stat,  step* 10 + step_accuracy_)\n",
    "        #print(\"acc\", s, e, start_true, end_true)\n",
    "        acc_accum += acc;\n",
    "        #print(\"acc:\", acc, \"Total:\", acc_accum)\n",
    "    last_avg_accuracy = acc_accum/acc_batch_size\n",
    "    writer.add_summary(stat_test,  step)\n",
    "    #writer.add_summary(stat,  step)\n",
    "    print('AVG accuracy', last_avg_accuracy)\n",
    "\n",
    "def trainStep(feed_dict, step, profiling = False):\n",
    "    global last_avg_accuracy\n",
    "    #start_true, end_true, doc, que, doc_v, que_v = sess.run(next_element)\n",
    "    if start_true < 0 or end_true > max_sequence_length - 1: \n",
    "        print('Ignore step', start_true, end_true)\n",
    "        return\n",
    "    \n",
    "    run_options = None\n",
    "    run_metadata = None\n",
    "    if profiling:\n",
    "        run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "        run_metadata = tf.RunMetadata()\n",
    "    _,loss, _, stat, stat_train = sess.run(\n",
    "        (train_step, sum_loss, accuracy,  summary_op, summary_op_train),\n",
    "        feed_dict = feed_dict,\n",
    "        options=run_options, run_metadata=run_metadata\n",
    "    )\n",
    "    #if step % 25 == 0 : print(step, loss, start_true, end_true)\n",
    "    if profiling: writer.add_run_metadata(run_metadata, 'step%d' % step)\n",
    "    writer.add_summary(stat, step)\n",
    "    writer.add_summary(stat_train, step)\n",
    "\n",
    "\n",
    "#dataset = ds.getDataset([\"./train_train_task_b.csv\"], max_sequence_length)\n",
    "#iterator = dataset.make_one_shot_iterator()\n",
    "#next_element = iterator.get_next()\n",
    "\n",
    "dataset_validation = ds.getDataset([\"./valid_dataset_160.csv\"], max_sequence_length)\n",
    "iterator_valid = dataset_validation.make_one_shot_iterator()\n",
    "next_element_valid = iterator_valid.get_next()\n",
    "\n",
    "summary_op_train = tf.summary.merge_all(\"TRAIN_STAT\")\n",
    "summary_op_test = tf.summary.merge_all(\"TEST_STAT\")\n",
    "summary_op = tf.summary.merge_all()\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    h_param_str = utils.make_h_param_string(FLAGS.learning_rate, FLAGS.lstm_size, max_sequence_length, FLAGS.maxout_pooling_size)\n",
    "    writer = tf.summary.FileWriter(FLAGS.log_path + \"/1-\" + h_param_str, sess.graph)\n",
    "    for epoch_ in range(200 or max_epoch):\n",
    "        #iterator = dataset.make_one_shot_iterator()\n",
    "        #next_element = iterator.get_next()\n",
    "        #iterator_valid = dataset_validation.make_one_shot_iterator()\n",
    "        #next_element_valid = iterator_valid.get_next()\n",
    "        #fout = open('train_dataset_processed.csv', 'w')\n",
    "        step_ = 0;\n",
    "        with open('train_dataset_160.csv') as inf:\n",
    "            for line in inf:\n",
    "                try: \n",
    "                    start_true, end_true, doc, que, doc_v, que_v = processLine(line, max_sequence_length, max_question_length)\n",
    "                    feed_dict = {\n",
    "                        question_ph: [que_v], \n",
    "                        document_ph: [doc_v], \n",
    "                        start_end_true: [start_true, end_true],\n",
    "                        doc_len_ph: len(doc),\n",
    "                        que_len_ph: len(que),\n",
    "                        avg_accuracy_ph: last_avg_accuracy,\n",
    "                        dropout_rate_ph: 0.5\n",
    "                    }\n",
    "                    profiling = False and True if (step_ % 51 == 0) else False\n",
    "                    trainStep(feed_dict, epoch_*50 + step_, profiling=profiling)\n",
    "\n",
    "                    if step_ > 0 and step_ % 100 == 0:\n",
    "                        # --------- ACCURACY -------------\n",
    "                        accuracyValidation(acc_batch_size, epoch_*9430 + step_)\n",
    "\n",
    "                    step_+=1\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    print(\"End of dataset\")  # ==> \"End of dataset\"\n",
    "                    break;\n",
    "                except: \n",
    "                    print('Error', \"skip\");\n",
    "                    step_+=1\n",
    "                if (step_ > 50): break;\n",
    "        print('Epoch', epoch_, 'completed')\n",
    "    print('End')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
