{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "#import maxout\n",
    "import highway_maxout as hmn\n",
    "import utils\n",
    "import dataset as ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#======= FLAGS ==========\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "tf.app.flags.DEFINE_integer('maxout_layer_size', 8, 'Maxout layer size')\n",
    "tf.app.flags.DEFINE_integer('maxout_pooling_size', 16, 'Maxout pooling size')\n",
    "tf.app.flags.DEFINE_integer('lstm_size', 200, 'LSTM cell internal size')\n",
    "tf.app.flags.DEFINE_string('log_path', '/tmp/dcn', 'logs location')\n",
    "tf.app.flags.DEFINE_integer('acc_batch_size', 5, 'How many examples to use to calculate accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove all variables\n",
    "tf.reset_default_graph();\n",
    "\n",
    "lstm_size = FLAGS.lstm_size\n",
    "acc_batch_size = FLAGS.acc_batch_size\n",
    "word_vector_size = 300\n",
    "maxout_pooling_size = FLAGS.maxout_pooling_size\n",
    "max_decoder_iterations = 4\n",
    "maxout_layer_size = FLAGS.maxout_layer_size;\n",
    "max_epoch = 1;\n",
    "max_sequence_length = 200\n",
    "#training_set_size = 100;\n",
    "\n",
    "# \n",
    "question_ph = tf.placeholder(tf.float32, [1, max_sequence_length, word_vector_size], name=\"q_input\")\n",
    "document_ph = tf.placeholder(tf.float32, [1, max_sequence_length, word_vector_size], name=\"d_input\")\n",
    "\n",
    "\n",
    "with tf.name_scope('ENCODER'):\n",
    "    # LSTM cell initialization\n",
    "    lstm = tf.nn.rnn_cell.LSTMCell(lstm_size)\n",
    "    lstm = tf.nn.rnn_cell.DropoutWrapper(cell=lstm, output_keep_prob=0.5)\n",
    "\n",
    "\n",
    "# LSTM cells for Bi-LSTM for COATINATION ENCODER\n",
    "with tf.name_scope('COATTENTION_ENCODER'):\n",
    "    lstm_cenc_fw = tf.nn.rnn_cell.LSTMCell(lstm_size)\n",
    "    lstm_cenc_fw = tf.nn.rnn_cell.DropoutWrapper(cell=lstm_cenc_fw, output_keep_prob=0.5)\n",
    "    lstm_cenc_bw = tf.nn.rnn_cell.LSTMCell(lstm_size)\n",
    "    lstm_cenc_bw = tf.nn.rnn_cell.DropoutWrapper(cell=lstm_cenc_bw, output_keep_prob=0.5)\n",
    "\n",
    "# create lstm cell for DYNAMIC POINTING DECODER\n",
    "lstm_dec = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "# get lstm initial state of zeroes\n",
    "#lstm_dec_state = lstm_dec.zero_state(1, tf.float32)\n",
    "start_pos = 0; # generate random between (0, document_size-1)\n",
    "end_pos = 0;   # generate random between (0, document_size-1)\n",
    "\n",
    "# create sentinel vector variable for both encodings \n",
    "#with tf.variable_scope(\"scope1\") as scope:\n",
    "sentinel_q = tf.get_variable(\"sentinel_q\", [ lstm_size , 1], initializer = tf.random_normal_initializer())\n",
    "sentinel_d = tf.get_variable(\"sentinel_d\", [ lstm_size , 1], initializer = tf.random_normal_initializer()) \n",
    "\n",
    "tf.summary.histogram('sentinel_q', sentinel_q)\n",
    "tf.summary.histogram('sentinel_q_max', tf.reduce_max(sentinel_q))\n",
    "tf.summary.histogram('sentinel_d', sentinel_d)\n",
    "tf.summary.histogram('sentinel_d_max', tf.reduce_max(sentinel_d))\n",
    "\n",
    "# optimizer\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Slice_1:0\", shape=(?, ?), dtype=float32)\n",
      "Tensor(\"COATTENTION_ENCODER_1/strided_slice:0\", shape=(?, 400), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'U_max:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# r = lstm(inputs = tf.convert_to_tensor([[1,2], [2,3]], dtype=tf.float32), state = zero_state_q)\n",
    "\n",
    "def length(sequence):\n",
    "  used = tf.sign(tf.reduce_max(tf.abs(sequence), 2))\n",
    "  length = tf.reduce_sum(used, 1)\n",
    "  length = tf.cast(length, tf.int32)\n",
    "  return length\n",
    "\n",
    "'''\n",
    "transform tensor of shape [1, question_size, word_vector_size] to list of tensors of shape [1, word_vector_size]\n",
    "of length question_size. first dimenstion is batch size = 1\n",
    "'''\n",
    "\n",
    "#print(tf.shape(question_ph)[1])\n",
    "#question_input = tf.unstack(question_ph, max_sequence_length, 1)\n",
    "#document_input = tf.unstack(document_ph, max_sequence_length, 1)\n",
    "#print(x)\n",
    "\n",
    "# we use the same LSTM for both encodings to share weights\n",
    "with tf.name_scope('ENCODER'):\n",
    "    with tf.name_scope('Q_ENC'):\n",
    "        outputs_q, state_q = tf.nn.dynamic_rnn(lstm, inputs = question_ph, sequence_length = length(question_ph), dtype=tf.float32)\n",
    "    with tf.name_scope('D_ENC'):\n",
    "        outputs_d, state_d = tf.nn.dynamic_rnn(lstm, inputs = document_ph, sequence_length = length(document_ph), dtype=tf.float32)\n",
    "\n",
    "\n",
    "document_size = length(document_ph)[0]\n",
    "question_size = length(question_ph)[0]\n",
    "doc_padding = tf.subtract([0, max_sequence_length], [0, document_size])\n",
    "que_padding = tf.subtract([0, max_sequence_length], [0, question_size])\n",
    "\n",
    "\n",
    "# \"squeeze\" transforms list of tensors of shape [1, lstm_size] of length L to tensor of shape [L, lstm_size]\n",
    "que_enc = tf.transpose(tf.squeeze(outputs_q))\n",
    "que_enc = tf.slice(que_enc, [0,0], [lstm_size, question_size])\n",
    "que_enc_sentinel = tf.concat([que_enc, sentinel_q], axis = 1)\n",
    "que_enc_sentinel = tf.pad(que_enc_sentinel, [[0,0], que_padding])\n",
    "que_enc_sentinel.set_shape([lstm_size, max_sequence_length + 1])\n",
    "que_enc_sentinel = utils.non_linear_projection(que_enc_sentinel)\n",
    "que_enc_sentinel = tf.slice(que_enc_sentinel, [0,0], [lstm_size, question_size + 1])\n",
    "#que_enc_sentinel.set_shape([lstm_size, max_sequence_length + 1])\n",
    "\n",
    "doc_enc = tf.transpose(tf.squeeze(outputs_d))\n",
    "doc_enc = tf.slice(doc_enc, [0,0], [lstm_size, document_size])\n",
    "#doc_enc = tf.pad(doc_enc, [[0,0], doc_padding])\n",
    "#doc_enc.set_shape([lstm_size, max_sequence_length])\n",
    "\n",
    "\n",
    "tf.summary.histogram('QUE_enc', que_enc)\n",
    "tf.summary.histogram('DOC_enc', doc_enc)\n",
    "tf.summary.histogram('DOC_enc_max', tf.reduce_max(doc_enc))\n",
    "tf.summary.histogram('QUE_enc_max', tf.reduce_max(que_enc))\n",
    "tf.summary.histogram('Document_size', document_size)\n",
    "tf.summary.histogram('Question_size', length(question_ph)[0])\n",
    "\n",
    "\n",
    "# append sentinel vector for both encodings \n",
    "doc_enc_sentinel = tf.concat([doc_enc, sentinel_d], axis = 1)\n",
    "#que_enc_sentinel = utils.non_linear_projection(tf.concat([que_enc, sentinel_q], axis = 1))\n",
    "print(que_enc_sentinel)\n",
    "#que_enc_sentinel = tf.slice(que_enc_sentinel, [0,0], [lstm_size, question_size + 1])\n",
    "\n",
    "# ===================  COATTENTION ENCODER ===================\n",
    "with tf.name_scope('COATTENTION_ENCODER'):\n",
    "    # L \\in R(doc_size + 1) x (que_size + 1)\n",
    "    L = tf.matmul(doc_enc_sentinel, que_enc_sentinel, transpose_a = True)\n",
    "    A_Q = tf.nn.softmax(L, 0)\n",
    "    A_D = tf.nn.softmax(tf.transpose(L), 1)\n",
    "    C_Q = tf.matmul(doc_enc_sentinel, A_Q)\n",
    "    # C_D \\in R_2*lstm_size x (doc_size + 1)\n",
    "    C_D = tf.matmul(tf.concat([que_enc_sentinel, C_Q], axis = 0), A_D)\n",
    "\n",
    "    # bi_lstm_input = tf.unstack(tf.reshape(tf.transpose(tf.concat([doc_enc_sentinel, C_D], axis = 0)), [max_sequence_length + 1, 1, 3*lstm_size]))\n",
    "    # TODO Q: would we use single cell of two different\n",
    "    bi_lstm_input = tf.concat([doc_enc_sentinel, C_D], axis = 0)\n",
    "    bi_lstm_input = tf.transpose(bi_lstm_input)\n",
    "    bi_lstm_input = tf.reshape(bi_lstm_input, [1, document_size + 1, 3*lstm_size])\n",
    "    \n",
    "    tf.summary.histogram('bi_lstm_input', bi_lstm_input)\n",
    "    \n",
    "    outputs_bi, output_state = tf.nn.bidirectional_dynamic_rnn(\n",
    "        cell_fw = lstm_cenc_fw, \n",
    "        cell_bw = lstm_cenc_bw,\n",
    "      #  cell_bw = lstm_cenc_bw,\n",
    "        inputs = bi_lstm_input,\n",
    "       # sequence_length = [document_size[0] + 1],\n",
    "        dtype=tf.float32\n",
    "    )\n",
    "\n",
    "    # we take first because of we feed to bi-RNN only one sentence\n",
    "    outputs_bi = tf.concat(outputs_bi, axis=2)[0]\n",
    "    print(outputs_bi)\n",
    "    U = tf.slice(outputs_bi, [0,0], [document_size, 2*lstm_size])\n",
    "    U = tf.transpose(U)\n",
    "#print(U)\n",
    "tf.summary.histogram('U', U)\n",
    "tf.summary.histogram('U_max', tf.reduce_max(U))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unknown>\n"
     ]
    }
   ],
   "source": [
    "# ===================== DYNAMIC POINTING DECODER =============\n",
    "\n",
    "\n",
    "#scope = tf.get_variable_scope()\n",
    "#u_t = get_scope_variable(scope, 'hmn_u_t', [2*lstm_size, 1]) \n",
    "#h_i = get_scope_variable(scope, 'hmn_h_i', [lstm_size, 1 ]) \n",
    "#u_s_i = get_scope_variable(scope, 'hmn_u_s_i', [2*lstm_size, 1])\n",
    "#u_e_i = get_scope_variable(scope, 'hmn_u_e_i', [2*lstm_size, 1])\n",
    "\n",
    "\n",
    "#m_3 = HMN(U, h_i, u_s_i, u_e_i)\n",
    "#print(m_3)\n",
    "\n",
    "# returns tuple (scores_start, scores_end, strart_pos, start_end, new_lstm_state)\n",
    "def decoderIteration(U, lstm_state, start_pos, end_pos):\n",
    "    with tf.name_scope('Decoder_Iteration'):\n",
    "        with tf.name_scope('Next_Start'):\n",
    "            scores_start = hmn.HMN(U, \n",
    "                               tf.transpose(lstm_state.h), \n",
    "                               tf.slice(U, [0, start_pos], [lstm_size*2, 1]) ,\n",
    "                               tf.slice(U, [0, end_pos], [lstm_size*2, 1]) , \n",
    "                               document_size,\n",
    "                               'start',\n",
    "                                FLAGS)\n",
    "\n",
    "            new_start_pos = tf.to_int32(tf.argmax(scores_start, 0))\n",
    "\n",
    "        #print(lstm_state)\n",
    "        with tf.name_scope('Next_End'):\n",
    "            scores_end = hmn.HMN(U, \n",
    "                             tf.transpose(lstm_state.h), \n",
    "                             tf.slice(U, [0, new_start_pos], [lstm_size*2, 1],) ,\n",
    "                             tf.slice(U, [0, end_pos], [lstm_size*2, 1]), \n",
    "                             document_size,\n",
    "                            'end',\n",
    "                            FLAGS)\n",
    "            new_end_pos = tf.to_int32(tf.argmax(scores_end, 0))\n",
    "        \n",
    "        with tf.name_scope('LSTM_State_Update'):\n",
    "            lstm_input = tf.concat(\n",
    "                [tf.slice(U, [0, new_start_pos], [lstm_size*2, 1], name='slice-5'), tf.slice(U, [0, new_end_pos], [lstm_size*2, 1])],\n",
    "                axis = 0\n",
    "            )\n",
    "            output, new_lstm_state = lstm_dec(tf.reshape(lstm_input, [1, lstm_size*4]), lstm_state)\n",
    "        \n",
    "        #print(new_lstm_state)\n",
    "        return scores_start, scores_end, new_start_pos , new_end_pos, new_lstm_state\n",
    "\n",
    "\n",
    "\n",
    "#print(lstm_dec_state)\n",
    "\n",
    "with tf.name_scope('DYNAMIC_POINTING_DECODER'):\n",
    "    \n",
    "    start_pos = 0;\n",
    "    end_pos = 0;\n",
    "    sum_start_scores = tf.zeros([1, document_size])\n",
    "    sum_end_scores = tf.zeros([1, document_size])\n",
    "    lstm_dec_state = lstm_dec.zero_state(1, tf.float32)\n",
    "    \n",
    "    for step in range(max_decoder_iterations):\n",
    "        scores_start, scores_end, new_start_pos, new_end_pos, lstm_dec_state = decoderIteration(U, lstm_dec_state, start_pos, end_pos)\n",
    "        sum_start_scores = tf.add(sum_start_scores, scores_start)\n",
    "        sum_end_scores   = tf.add(sum_end_scores, scores_end)\n",
    "        if new_start_pos == start_pos and end_pos == new_end_pos : break\n",
    "        start_pos = new_start_pos\n",
    "        end_pos = new_end_pos\n",
    "\n",
    "    \n",
    "# loss and train step\n",
    "start_end_true = tf.placeholder(tf.int32, [2]);\n",
    "#end_true = tf.placeholder(tf.int32, ());\n",
    "onehot_labels = tf.one_hot(start_end_true, document_size)\n",
    "with tf.name_scope('Loss'):\n",
    "    sum_loss = tf.losses.softmax_cross_entropy(\n",
    "        onehot_labels,\n",
    "        tf.concat([sum_start_scores, sum_end_scores], axis=0))\n",
    "\n",
    "\n",
    "    \n",
    "with tf.name_scope('Accuracy'):\n",
    "    with tf.name_scope('Prediction'):\n",
    "        pr_start_idx = tf.to_int32(tf.argmax(sum_start_scores, 0))[0]\n",
    "        pr_end_idx = tf.to_int32(tf.argmax(sum_end_scores, 0))[0]\n",
    "    with tf.name_scope('Accuracy'):\n",
    "        accuracy = tf.py_func(utils.f1_score_int, [pr_start_idx, pr_end_idx, start_end_true[0], start_end_true[1]], tf.float64)\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "    \n",
    "print(sum_start_scores.get_shape())    \n",
    "    \n",
    "tf.summary.scalar('loss', sum_loss)\n",
    "with tf.name_scope('Train'):\n",
    "    train_step = optimizer.minimize(sum_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVG accuracy 0.0\n",
      "10 5.17778\n",
      "20 5.53485\n",
      "30 2.10899\n",
      "40 4.63338\n",
      "AVG accuracy 0.0\n",
      "60 3.68043\n",
      "70 4.30367\n",
      "80 5.95793\n",
      "90 5.43688\n",
      "AVG accuracy 0.0\n",
      "110 4.87964\n",
      "120 3.68106\n",
      "130 0.0\n",
      "140 4.99858\n",
      "AVG accuracy 0.0\n",
      "160 3.73751\n",
      "170 4.67802\n",
      "180 4.65649\n",
      "190 3.87879\n",
      "AVG accuracy 0.2\n",
      "210 5.01349\n",
      "220 4.32025\n",
      "230 5.20695\n",
      "240 4.78314\n",
      "AVG accuracy 0.0\n",
      "260 4.7131\n",
      "270 4.94823\n",
      "280 3.87943\n",
      "290 0.0\n",
      "AVG accuracy 0.0\n",
      "310 7.83369\n",
      "320 2.74057\n",
      "330 5.23961\n",
      "340 0.0\n",
      "AVG accuracy 0.0\n",
      "360 3.65625\n",
      "370 5.36095\n",
      "380 4.96452\n",
      "390 4.99494\n",
      "AVG accuracy 0.0\n",
      "410 3.5628\n",
      "420 4.47711\n",
      "430 4.28279\n",
      "440 4.57718\n",
      "AVG accuracy 0.0\n",
      "460 4.39298\n",
      "470 5.35128\n",
      "480 5.72877\n",
      "490 5.5701\n",
      "AVG accuracy 0.0\n",
      "510 4.65971\n",
      "520 3.83323\n",
      "530 3.48888\n",
      "540 4.92157\n",
      "AVG accuracy 0.04\n",
      "560 6.01977\n",
      "570 7.11768\n",
      "580 5.88922\n",
      "590 3.30145\n",
      "AVG accuracy 0.0\n",
      "610 1.65148\n",
      "620 4.65765\n",
      "630 5.22062\n",
      "640 5.32525\n",
      "AVG accuracy 0.0\n",
      "660 4.1202\n",
      "670 4.90649\n",
      "680 5.60851\n",
      "690 4.64501\n",
      "AVG accuracy 0.0\n",
      "710 4.80811\n",
      "720 4.43369\n",
      "730 3.9217\n",
      "740 3.69645\n",
      "AVG accuracy 0.0\n",
      "760 5.06347\n",
      "770 4.63376\n",
      "780 4.84498\n",
      "790 5.38423\n",
      "AVG accuracy 0.0\n",
      "810 5.1849\n",
      "820 5.60627\n",
      "830 4.86497\n",
      "840 4.62425\n",
      "AVG accuracy 0.0\n",
      "860 5.11245\n",
      "870 5.48891\n",
      "880 4.72796\n",
      "890 4.44732\n",
      "AVG accuracy 0.0\n",
      "910 5.69391\n",
      "920 4.55284\n",
      "Ignore step 297 298\n",
      "930 4.81136\n",
      "940 3.26619\n",
      "AVG accuracy 0.0\n",
      "960 4.99632\n",
      "970 4.80856\n",
      "980 5.72054\n",
      "990 4.51404\n",
      "AVG accuracy 0.213333333333\n",
      "1010 4.67822\n",
      "1020 4.93059\n",
      "1030 8.48569\n",
      "1040 5.32773\n",
      "AVG accuracy 0.0\n",
      "1060 4.37492\n",
      "1910 4.23225\n",
      "1920 4.44656\n",
      "1930 5.00517\n",
      "1940 3.85287\n",
      "AVG accuracy 0.0571428571429\n",
      "1960 4.72168\n",
      "1970 4.19344\n",
      "1980 4.79928\n",
      "1990 5.26158\n",
      "AVG accuracy 0.08\n",
      "2010 5.5441\n",
      "2020 2.12117\n",
      "2030 4.42392\n",
      "Ignore step 312 314\n",
      "AVG accuracy 0.0\n",
      "2060 4.15481\n",
      "2070 4.27717\n",
      "2080 4.92472\n",
      "2090 4.49195\n",
      "AVG accuracy 0.0\n",
      "2110 4.26394\n",
      "2120 4.42807\n",
      "2130 4.80543\n",
      "2140 4.87017\n",
      "AVG accuracy 0.0\n",
      "2160 4.38976\n",
      "2170 3.98365\n",
      "2180 4.21109\n",
      "2190 4.44371\n",
      "AVG accuracy 0.0\n",
      "2210 4.44928\n",
      "2220 4.40923\n",
      "2230 4.9168\n",
      "2240 5.31136\n",
      "AVG accuracy 0.0\n",
      "2260 4.67533\n",
      "2270 4.42524\n",
      "2280 4.71401\n",
      "2290 0.0\n",
      "AVG accuracy 0.0\n",
      "2310 4.81071\n",
      "2320 4.96832\n",
      "2330 6.36448\n",
      "2340 4.15451\n",
      "AVG accuracy 0.0\n",
      "2360 4.53711\n",
      "2370 4.71906\n",
      "2380 4.62627\n",
      "2390 3.88693\n",
      "AVG accuracy 0.0\n",
      "2410 4.44432\n",
      "2420 3.37031\n",
      "2430 4.25286\n",
      "2440 4.06311\n",
      "AVG accuracy 0.0\n",
      "2460 5.13766\n",
      "2470 4.11286\n",
      "2480 4.88941\n",
      "2490 4.74565\n",
      "AVG accuracy 0.0\n",
      "2510 5.72227\n",
      "2520 4.40991\n",
      "2530 4.01003\n",
      "2540 4.51508\n",
      "AVG accuracy 0.0\n",
      "2560 4.56641\n",
      "2570 3.94972\n",
      "2580 5.42477\n",
      "2590 3.97814\n",
      "AVG accuracy 0.0571428571429\n",
      "2610 4.82867\n",
      "2620 5.15253\n",
      "2630 4.76728\n",
      "2640 4.11694\n",
      "AVG accuracy 0.0\n",
      "2660 4.47879\n",
      "2670 4.13112\n",
      "2680 5.00309\n",
      "2690 4.92727\n",
      "AVG accuracy 0.1\n",
      "2710 5.32314\n",
      "2720 3.86709\n",
      "2730 4.47513\n",
      "2740 4.65365\n",
      "AVG accuracy 0.0\n",
      "2760 4.23812\n",
      "2770 4.18105\n",
      "2780 5.2021\n",
      "2790 4.49012\n",
      "AVG accuracy 0.0\n",
      "2810 4.90285\n",
      "2820 3.95855\n",
      "2830 4.2856\n",
      "2840 5.54298\n",
      "AVG accuracy 0.0\n",
      "2860 6.40482\n",
      "2870 6.4475\n",
      "2880 4.99044\n",
      "2890 4.28462\n",
      "AVG accuracy 0.0\n",
      "2910 13.6834\n",
      "2920 4.21799\n",
      "2930 5.09089\n",
      "2940 5.52189\n",
      "AVG accuracy 0.4\n",
      "2960 4.05341\n",
      "2970 4.08239\n",
      "2980 4.83144\n",
      "2990 3.81468\n",
      "AVG accuracy 0.2\n",
      "3010 4.39374\n",
      "3020 4.73521\n",
      "3030 17.744\n",
      "3040 5.95318\n",
      "AVG accuracy 0.0\n",
      "3060 4.40831\n",
      "3070 3.78927\n",
      "3080 4.852\n",
      "3090 5.15959\n",
      "AVG accuracy 0.0\n",
      "3110 1.5445\n",
      "3120 5.13311\n",
      "3130 7.47828\n",
      "3140 0.0\n",
      "AVG accuracy 0.0\n",
      "3160 5.21372\n",
      "3170 4.27018\n",
      "3180 5.18287\n",
      "3190 4.76986\n",
      "AVG accuracy 0.0\n",
      "3210 5.47359\n",
      "3220 4.18566\n",
      "3230 5.20193\n",
      "Ignore step 226 235\n",
      "3240 6.08596\n",
      "AVG accuracy 0.0\n",
      "3260 4.52783\n",
      "3270 3.89496\n",
      "3280 5.10288\n",
      "Ignore step -1 -1\n",
      "3290 4.90084\n",
      "AVG accuracy 0.0\n",
      "3310 4.58568\n",
      "3320 4.78111\n",
      "3330 5.1815\n",
      "3340 3.42107\n",
      "AVG accuracy 0.0\n",
      "3360 4.42563\n",
      "3370 5.13523\n",
      "3380 3.84351\n",
      "3390 5.00989\n",
      "AVG accuracy 0.0\n",
      "3410 3.76081\n",
      "3420 11.2817\n",
      "3430 5.05765\n",
      "3440 5.97946\n",
      "AVG accuracy 0.0\n",
      "3460 5.2152\n",
      "3470 5.37979\n",
      "3480 4.02928\n",
      "3490 4.89271\n",
      "AVG accuracy 0.133333333333\n",
      "3510 5.81114\n",
      "3520 5.55762\n",
      "3530 10.5784\n",
      "3540 5.26319\n",
      "AVG accuracy 0.0\n",
      "3560 11.4637\n",
      "Ignore step -1 -1\n",
      "3570 6.50703\n",
      "3580 4.31889\n",
      "3590 5.09794\n",
      "AVG accuracy 0.0\n",
      "3610 6.55139\n",
      "3620 2.71789\n",
      "3630 6.50694\n",
      "3640 5.00394\n",
      "AVG accuracy 0.0\n",
      "3660 4.82998\n",
      "3670 5.21003\n",
      "3680 8.50157\n",
      "3690 5.67592\n",
      "AVG accuracy 0.0\n",
      "3710 5.63735\n",
      "3720 4.52648\n",
      "3730 2.19261\n",
      "3740 4.95875\n",
      "AVG accuracy 0.266666666667\n",
      "3760 4.97687\n",
      "3770 4.00055\n",
      "3780 4.00605\n",
      "3790 4.83265\n",
      "AVG accuracy 0.0\n",
      "3810 0.0\n",
      "3820 4.05872\n",
      "3830 4.55364\n",
      "3840 3.09601\n",
      "AVG accuracy 0.0\n",
      "3860 5.13652\n",
      "3870 5.64186\n",
      "3880 3.93522\n",
      "3890 4.4097\n",
      "Ignore step -1 -1\n",
      "AVG accuracy 0.1\n",
      "Ignore step 208 210\n",
      "3910 4.80093\n",
      "3920 5.50742\n",
      "Ignore step -1 -1\n",
      "3930 5.69895\n",
      "3940 0.0\n",
      "AVG accuracy 0.08\n",
      "Ignore step -1 -1\n",
      "3960 6.38696\n",
      "3970 6.23969\n",
      "3980 4.34372\n",
      "3990 4.949\n",
      "AVG accuracy 0.0\n",
      "4010 5.62663\n",
      "4020 6.28056\n",
      "4030 4.69124\n",
      "4040 5.56052\n",
      "AVG accuracy 0.0\n",
      "4060 6.5071\n",
      "4070 5.37524\n",
      "4080 4.37167\n",
      "4090 5.61314\n",
      "AVG accuracy 0.0\n",
      "4110 3.91242\n",
      "4120 4.99014\n",
      "4130 5.38427\n",
      "4140 5.13203\n",
      "AVG accuracy 0.0\n",
      "4160 3.98798\n",
      "4170 4.413\n",
      "4180 5.21732\n",
      "4190 5.46141\n",
      "AVG accuracy 0.0\n",
      "4210 4.24988\n",
      "4220 6.09814\n",
      "4230 6.02544\n",
      "4240 4.94136\n",
      "AVG accuracy 0.0\n",
      "4260 5.85077\n",
      "4270 7.68287\n",
      "4280 4.24764\n",
      "4290 0.0\n",
      "AVG accuracy 0.0\n",
      "4310 12.9801\n",
      "4320 4.43834\n",
      "4330 5.28194\n",
      "4340 10.1944\n",
      "AVG accuracy 0.0\n",
      "4360 5.68638\n",
      "4370 2.04325\n",
      "4380 8.55589\n",
      "4390 4.55145\n",
      "AVG accuracy 0.133333333333\n",
      "4410 8.39598\n",
      "4420 6.23997\n",
      "4430 6.38975\n",
      "4440 7.43583\n",
      "AVG accuracy 0.0\n",
      "4460 4.43124\n",
      "4470 5.39839\n",
      "4480 4.18401\n",
      "Ignore step -1 -1\n",
      "4490 5.6104\n",
      "AVG accuracy 0.0\n",
      "4510 3.69488\n",
      "4520 4.38474\n",
      "4530 6.06542\n",
      "4540 6.75004\n",
      "AVG accuracy 0.0\n",
      "4560 5.54428\n",
      "4570 7.49773\n",
      "4580 7.07532\n",
      "4590 5.3134\n",
      "AVG accuracy 0.166666666667\n",
      "4610 4.41055\n",
      "Ignore step 201 208\n",
      "4620 3.94031\n",
      "4630 7.31235\n",
      "4640 7.97562\n",
      "AVG accuracy 0.0\n",
      "4660 7.36069\n",
      "4670 6.51922\n",
      "4680 7.52866\n",
      "4690 4.98001\n",
      "Ignore step 394 398\n",
      "AVG accuracy 0.0\n",
      "4710 4.68521\n",
      "4720 4.12593\n",
      "4730 4.71739\n",
      "4740 3.45521\n",
      "AVG accuracy 0.133333333333\n",
      "4760 4.40043\n",
      "4770 3.79563\n",
      "4780 4.95364\n",
      "4790 4.17765\n",
      "AVG accuracy 0.133333333333\n",
      "4810 3.23849\n",
      "4820 4.8548\n",
      "4830 7.19254\n",
      "4840 4.60292\n",
      "AVG accuracy 0.0\n",
      "4860 4.66795\n",
      "4870 2.66005\n",
      "4880 0.0\n",
      "4890 8.69601\n",
      "AVG accuracy 0.0\n",
      "4910 8.81818\n",
      "4920 6.39318\n",
      "4930 6.12856\n",
      "4940 4.24795\n",
      "AVG accuracy 0.0\n",
      "4960 5.6237\n",
      "4970 4.84913\n",
      "4980 5.24139\n",
      "4990 4.44948\n",
      "AVG accuracy 0.0\n",
      "5010 9.63503\n",
      "5020 4.54486\n",
      "5030 8.68601\n",
      "5040 5.35564\n",
      "AVG accuracy 0.266666666667\n",
      "5060 5.12543\n",
      "5070 5.7087\n",
      "5080 5.77785\n",
      "5090 4.15886\n",
      "AVG accuracy 0.0\n",
      "5110 3.98355\n",
      "5120 4.42906\n",
      "5130 4.58701\n",
      "5140 3.84215\n",
      "AVG accuracy 0.0\n",
      "5160 3.71454\n",
      "5170 4.31411\n",
      "5180 5.70853\n",
      "5190 4.25558\n",
      "AVG accuracy 0.0\n",
      "5210 4.4772\n",
      "5220 0.0\n",
      "5230 5.62146\n",
      "5240 5.81862\n",
      "AVG accuracy 0.0\n",
      "5260 4.73007\n",
      "5270 5.95711\n",
      "5280 4.92425\n",
      "5290 5.7295\n",
      "AVG accuracy 0.133333333333\n",
      "5310 4.71155\n",
      "5320 2.59325\n",
      "5330 7.48035\n",
      "5340 5.598\n",
      "AVG accuracy 0.0\n",
      "Ignore step 235 238\n",
      "5360 3.69967\n",
      "5370 4.59459\n",
      "5380 7.56483\n",
      "5390 6.51291\n",
      "AVG accuracy 0.0\n",
      "5410 5.38285\n",
      "5420 10.1577\n",
      "5430 8.09252\n",
      "5440 2.73212\n",
      "AVG accuracy 0.0\n",
      "5460 9.56657\n",
      "Ignore step -1 -1\n",
      "5470 2.38938\n",
      "5480 7.43198\n",
      "5490 4.86513\n",
      "AVG accuracy 0.0\n",
      "5510 4.18861\n",
      "Ignore step 202 203\n",
      "5520 4.60949\n",
      "5530 5.40923\n",
      "5540 6.52468\n",
      "AVG accuracy 0.2\n",
      "5560 2.44036\n",
      "5570 0.0\n",
      "5580 10.9987\n",
      "5590 2.52997\n",
      "AVG accuracy 0.0\n",
      "5610 3.7614\n",
      "5620 5.10624\n",
      "5630 8.16816\n",
      "5640 8.41073\n",
      "AVG accuracy 0.0\n",
      "5660 4.83854\n",
      "5670 6.49568\n",
      "5680 12.1861\n",
      "5690 11.1496\n",
      "AVG accuracy 0.0\n",
      "5710 12.225\n",
      "5720 4.53514\n",
      "5730 4.31416\n",
      "5740 4.58661\n",
      "AVG accuracy 0.0666666666667\n",
      "5760 10.4045\n",
      "5770 8.95441\n",
      "5780 6.98806\n",
      "5790 5.44821\n",
      "AVG accuracy 0.0\n",
      "5810 5.71029\n",
      "5820 4.85513\n",
      "Ignore step 224 228\n",
      "5830 5.34983\n",
      "5840 6.36415\n",
      "AVG accuracy 0.08\n",
      "5860 4.36324\n",
      "5870 4.12115\n",
      "5880 5.0782\n",
      "5890 9.97229\n",
      "AVG accuracy 0.1\n",
      "5910 4.36006\n",
      "5920 5.96036\n",
      "5930 6.67546\n",
      "5940 7.40245\n",
      "AVG accuracy 0.0666666666667\n",
      "5960 5.53836\n",
      "5970 10.5281\n",
      "Ignore step 195 201\n",
      "Ignore step -1 -1\n",
      "5990 5.61933\n",
      "AVG accuracy 0.0\n",
      "6010 5.30006\n",
      "6020 5.48821\n",
      "6030 4.78575\n",
      "6040 4.29852\n",
      "AVG accuracy 0.0\n",
      "6060 10.7875\n",
      "6070 4.04171\n",
      "6080 4.71314\n",
      "6090 5.87178\n",
      "AVG accuracy 0.0\n",
      "6110 5.18096\n",
      "6120 11.5476\n",
      "6130 6.6871\n",
      "6140 7.16739\n",
      "AVG accuracy 0.0\n",
      "6160 7.72898\n",
      "6170 4.0391\n",
      "6180 4.72448\n",
      "6190 5.31732\n",
      "AVG accuracy 0.0\n",
      "6210 5.95904\n",
      "6220 7.04105\n",
      "6230 4.32017\n",
      "6240 7.83654\n",
      "AVG accuracy 0.0\n",
      "6260 6.42785\n",
      "6270 3.89124\n",
      "6280 4.2843\n",
      "6290 0.0\n",
      "AVG accuracy 0.1\n",
      "Ignore step -1 -1\n",
      "6310 6.3697\n",
      "6320 5.38542\n",
      "6330 4.19439\n",
      "6340 5.2359\n",
      "AVG accuracy 0.0\n",
      "6360 9.82104\n",
      "6370 10.0015\n",
      "6380 9.31849\n",
      "6390 5.75317\n",
      "AVG accuracy 0.0\n",
      "6410 10.6172\n",
      "6420 6.22329\n",
      "6430 14.3035\n",
      "6440 14.1782\n",
      "AVG accuracy 0.0\n",
      "6460 6.07524\n",
      "6470 4.95729\n",
      "6480 11.3495\n",
      "6490 13.5408\n",
      "AVG accuracy 0.0\n",
      "6510 13.5806\n",
      "6520 13.5678\n",
      "6530 6.84096\n",
      "6540 12.4311\n",
      "AVG accuracy 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6560 3.39595\n",
      "6570 3.20662\n",
      "6580 7.17646\n",
      "6590 4.6097\n",
      "AVG accuracy 0.0\n",
      "6610 6.12104\n",
      "Ignore step 263 264\n",
      "6620 4.42321\n",
      "Ignore step 211 212\n",
      "6630 4.88855\n",
      "6640 5.00795\n",
      "AVG accuracy 0.0\n",
      "6660 4.63044\n",
      "6670 5.10181\n",
      "6680 5.5078\n",
      "6690 6.99984\n",
      "AVG accuracy 0.133333333333\n",
      "6710 6.66776\n",
      "6720 5.51855\n",
      "6730 5.66182\n",
      "6740 5.12086\n",
      "AVG accuracy 0.0\n",
      "6760 2.30864\n",
      "6770 3.50421\n",
      "6780 4.47638\n",
      "6790 7.60894\n",
      "AVG accuracy 0.0\n",
      "6810 5.05059\n",
      "6820 17.141\n",
      "6830 9.21461\n",
      "6840 9.25793\n",
      "AVG accuracy 0.0\n",
      "6860 8.34832\n",
      "6870 10.4835\n",
      "6880 7.50755\n",
      "6890 7.27294\n",
      "AVG accuracy 0.2\n",
      "6910 9.01362\n",
      "6920 4.32329\n",
      "6930 7.91559\n",
      "6940 2.13464\n",
      "AVG accuracy 0.0\n",
      "6960 6.93843\n",
      "6970 6.81331\n",
      "6980 5.76453\n",
      "6990 7.14445\n",
      "AVG accuracy 0.0\n",
      "7010 9.64224\n",
      "7020 6.6603\n",
      "7030 6.03469\n",
      "7040 7.0124\n",
      "Ignore step 222 230\n",
      "Ignore step -1 -1\n",
      "AVG accuracy 0.0666666666667\n",
      "Ignore step -1 -1\n",
      "7060 4.87609\n",
      "7070 6.47144\n",
      "Ignore step 244 249\n",
      "7080 6.02363\n",
      "7090 4.60376\n",
      "AVG accuracy 0.0\n",
      "7110 5.65949\n",
      "7120 5.40956\n",
      "7130 5.52999\n",
      "7140 4.684\n",
      "AVG accuracy 0.0\n",
      "7160 3.65089\n",
      "7170 5.12106\n",
      "7180 6.49741\n",
      "7190 4.27753\n",
      "AVG accuracy 0.0\n",
      "7210 2.17859\n",
      "7220 20.1568\n",
      "7230 0.0\n",
      "7240 5.00099\n",
      "AVG accuracy 0.0\n",
      "7260 4.52439\n",
      "7270 6.09538\n",
      "7280 6.88203\n",
      "7290 0.0\n",
      "AVG accuracy 0.0\n",
      "7310 0.0\n",
      "7320 14.6767\n",
      "7330 6.23643\n",
      "7340 13.0103\n",
      "AVG accuracy 0.1\n",
      "Ignore step -1 -1\n",
      "7360 14.4692\n",
      "7370 23.739\n",
      "7380 41.9192\n",
      "7390 44.6269\n",
      "AVG accuracy 0.0\n",
      "7410 4.96223\n",
      "7420 15.674\n",
      "7430 19.6063\n",
      "7440 26.6452\n",
      "AVG accuracy 0.0\n",
      "7460 29.0133\n",
      "7470 21.0191\n",
      "7480 38.4995\n",
      "7490 6.40129\n",
      "AVG accuracy 0.0\n",
      "7510 11.0261\n",
      "7520 7.67675\n",
      "7530 14.8708\n",
      "7540 5.8285\n",
      "AVG accuracy 0.0\n",
      "7560 4.52101\n",
      "7570 8.43829\n",
      "7580 0.0\n",
      "7590 6.52186\n",
      "AVG accuracy 0.0\n",
      "7610 3.26575\n",
      "7620 7.04488\n",
      "7630 7.4869\n",
      "7640 9.21634\n",
      "AVG accuracy 0.0\n",
      "7660 0.0\n",
      "7670 0.0\n",
      "7680 11.6616\n",
      "7690 32.0576\n",
      "AVG accuracy 0.1\n",
      "7710 20.0716\n",
      "7720 12.3894\n",
      "7730 9.7576\n",
      "Ignore step 223 227\n",
      "7740 5.58469\n",
      "AVG accuracy 0.0\n",
      "7760 19.4448\n",
      "7770 5.1772\n",
      "7780 6.03464\n",
      "7790 4.50269\n",
      "AVG accuracy 0.0\n",
      "7810 6.38386\n",
      "7820 5.83145\n",
      "7830 5.00585\n",
      "7840 4.94713\n",
      "AVG accuracy 0.0\n",
      "7860 3.71692\n",
      "7870 3.70705\n",
      "7880 6.48337\n",
      "7890 2.36101\n",
      "AVG accuracy 0.0\n",
      "7910 9.29681\n",
      "7920 7.624\n",
      "7930 1.3167\n",
      "7940 4.92289\n",
      "AVG accuracy 0.0\n",
      "7960 0.0\n",
      "7970 6.87015\n",
      "7980 5.86035\n",
      "7990 9.94711\n",
      "AVG accuracy 0.0\n",
      "8010 8.83068\n",
      "8020 6.67273\n",
      "Ignore step -1 -1\n",
      "8030 8.87787\n",
      "8040 8.39088\n",
      "AVG accuracy 0.0\n",
      "8060 0.0\n",
      "8070 2.71559\n",
      "8080 19.1022\n",
      "8090 13.3679\n",
      "AVG accuracy 0.0\n",
      "8110 11.6434\n",
      "8120 6.70627\n",
      "8130 13.3517\n",
      "8140 17.594\n",
      "AVG accuracy 0.0\n",
      "8160 6.0542\n",
      "8170 7.53263\n",
      "8180 13.8576\n",
      "8190 24.927\n",
      "AVG accuracy 0.0\n",
      "8210 14.2418\n",
      "8220 15.2723\n",
      "8230 8.69597\n",
      "8240 8.05084\n",
      "AVG accuracy 0.0\n",
      "8260 7.08586\n",
      "8270 4.62195\n",
      "8280 4.24794\n",
      "Ignore step 220 223\n",
      "8290 6.78357\n",
      "AVG accuracy 0.0\n",
      "8310 6.05005\n",
      "8320 5.51059\n",
      "8330 5.25735\n",
      "8340 6.53833\n",
      "AVG accuracy 0.0\n",
      "8360 8.66985\n",
      "Ignore step -1 -1\n",
      "8370 8.735\n",
      "8380 10.623\n",
      "8390 9.47659\n",
      "Ignore step 472 473\n",
      "AVG accuracy 0.0\n",
      "8410 6.81093\n",
      "Ignore step 237 239\n",
      "8430 9.93511\n",
      "8440 9.40358\n",
      "AVG accuracy 0.08\n",
      "8460 5.47373\n",
      "Ignore step 203 204\n",
      "8470 13.6329\n",
      "8480 3.92558\n",
      "Ignore step -1 -1\n",
      "8490 20.7064\n",
      "Ignore step -1 -1\n",
      "AVG accuracy 0.0\n",
      "8510 4.52678\n",
      "Ignore step -1 -1\n",
      "8530 0.0\n",
      "8540 6.07083\n",
      "AVG accuracy 0.0\n",
      "8560 5.41962\n",
      "Ignore step 279 282\n",
      "8570 5.45621\n",
      "8580 8.01276\n",
      "8590 6.20814\n",
      "AVG accuracy 0.08\n",
      "8610 3.53869\n",
      "8620 8.00286\n",
      "8630 9.80078\n",
      "8640 10.0889\n",
      "AVG accuracy 0.0\n",
      "8660 7.17847\n",
      "8670 8.80529\n",
      "8680 6.95254\n",
      "Ignore step 205 206\n",
      "8690 6.85852\n",
      "AVG accuracy 0.0\n",
      "8710 2.47671\n",
      "8720 6.92632\n",
      "8730 8.14184\n",
      "8740 5.12551\n",
      "AVG accuracy 0.0\n",
      "8760 3.85583\n",
      "8770 3.77534\n",
      "8780 4.4196\n",
      "8790 3.98194\n",
      "AVG accuracy 0.0\n",
      "8810 5.82313\n",
      "8820 7.28955\n",
      "8830 5.29317\n",
      "8840 5.01459\n",
      "AVG accuracy 0.1\n",
      "8860 4.83536\n",
      "8870 2.29358\n",
      "8880 6.21647\n",
      "8890 5.22516\n",
      "AVG accuracy 0.0\n",
      "8910 3.76851\n",
      "8920 6.4445\n",
      "8930 8.73023\n",
      "Ignore step 227 230\n",
      "8940 7.93865\n",
      "AVG accuracy 0.0\n",
      "8960 10.6686\n",
      "8970 6.88794\n",
      "8980 7.85287\n",
      "8990 6.52164\n",
      "AVG accuracy 0.2\n",
      "9010 5.90186\n",
      "9020 4.99685\n",
      "9030 9.55068\n",
      "9040 2.42932\n",
      "AVG accuracy 0.133333333333\n",
      "9060 0.560803\n",
      "Ignore step -1 -1\n",
      "9080 10.0379\n",
      "9090 10.0958\n",
      "AVG accuracy 0.0\n",
      "9110 1.45307\n",
      "Ignore step -1 -1\n",
      "9120 9.02025\n",
      "9130 9.33854\n",
      "9140 1.97711\n",
      "AVG accuracy 0.1\n",
      "9160 7.6439\n",
      "9170 11.2753\n",
      "9180 6.10672\n",
      "9190 4.2189\n",
      "AVG accuracy 0.0\n",
      "9210 7.55649\n",
      "9220 8.13314\n",
      "9230 5.38894\n",
      "9240 8.72616\n",
      "AVG accuracy 0.0\n",
      "9260 5.9935\n",
      "9270 13.6285\n",
      "9280 18.1171\n",
      "9290 8.95465\n",
      "Ignore step -1 -1\n",
      "AVG accuracy 0.0\n",
      "9310 10.0269\n",
      "Ignore step 228 236\n",
      "9330 21.2753\n",
      "9340 10.0962\n",
      "AVG accuracy 0.0\n",
      "9360 10.6199\n",
      "9370 6.72548\n",
      "9380 12.8815\n",
      "9390 5.42521\n",
      "AVG accuracy 0.1\n",
      "9410 11.0254\n",
      "9420 10.775\n",
      "9430 8.67657\n",
      "9440 10.609\n",
      "AVG accuracy 0.0\n",
      "9460 4.8488\n",
      "9470 5.2027\n",
      "9480 3.52445\n",
      "9490 4.53847\n",
      "AVG accuracy 0.0\n",
      "9510 6.76772\n",
      "9520 4.97848\n",
      "9530 20.7083\n",
      "9540 23.4076\n",
      "AVG accuracy 0.133333333333\n",
      "9560 9.279\n",
      "9570 7.95203\n",
      "9580 4.43057\n",
      "9590 11.4178\n",
      "AVG accuracy 0.0\n",
      "9610 10.3471\n",
      "9620 6.43448\n",
      "9630 10.9021\n",
      "9640 4.83157\n",
      "AVG accuracy 0.0\n",
      "9660 4.29446\n",
      "9670 7.22922\n",
      "9680 6.22317\n",
      "9690 7.41112\n",
      "AVG accuracy 0.2\n",
      "9710 3.83449\n",
      "9720 7.77605\n",
      "9730 8.50696\n",
      "9740 7.00735\n",
      "AVG accuracy 0.2\n",
      "9760 13.8745\n",
      "9770 13.0729\n",
      "9780 6.23016\n",
      "9790 9.50186\n",
      "AVG accuracy 0.0\n",
      "9810 6.08072\n",
      "9820 22.7134\n",
      "9830 14.0052\n",
      "9840 8.26818\n",
      "AVG accuracy 0.0\n",
      "9860 25.7589\n",
      "9870 12.1516\n",
      "9880 15.8339\n",
      "9890 14.6929\n",
      "AVG accuracy 0.0\n",
      "9910 5.74366\n",
      "9920 4.85228\n",
      "9930 6.84339\n",
      "9940 34.2785\n",
      "AVG accuracy 0.0\n",
      "9960 37.041\n",
      "Ignore step 219 224\n",
      "9970 7.84285\n",
      "9980 15.3653\n",
      "9990 6.60359\n",
      "Ignore step 278 280\n",
      "AVG accuracy 0.133333333333\n",
      "10010 8.18239\n",
      "10020 3.93483\n",
      "10030 30.5577\n",
      "Ignore step 206 209\n",
      "10040 14.3346\n",
      "AVG accuracy 0.0\n",
      "10060 32.4634\n",
      "10070 54.7864\n",
      "10080 4.62135\n",
      "10090 9.45738\n",
      "AVG accuracy 0.0\n",
      "10110 19.8076\n",
      "10120 20.2923\n",
      "10130 11.9414\n",
      "10140 7.95982\n",
      "AVG accuracy 0.0\n",
      "10160 0.0\n",
      "10170 39.9819\n",
      "10180 12.0657\n",
      "10190 14.115\n",
      "AVG accuracy 0.1\n",
      "10210 8.13381\n",
      "10220 4.08507\n",
      "10230 5.15316\n",
      "10240 6.37859\n",
      "AVG accuracy 0.0\n",
      "10260 5.23598\n",
      "10270 8.32211\n",
      "10280 14.029\n",
      "10290 9.22887\n",
      "AVG accuracy 0.0\n",
      "10310 9.92372\n",
      "10320 11.23\n",
      "10330 12.4172\n",
      "10340 2.74822\n",
      "AVG accuracy 0.0\n",
      "10360 9.05\n",
      "10370 28.4322\n",
      "10380 9.76087\n",
      "10390 16.3401\n",
      "AVG accuracy 0.0\n",
      "10410 18.1645\n",
      "10420 13.0823\n",
      "10430 10.2801\n",
      "10440 11.7682\n",
      "AVG accuracy 0.0\n",
      "10460 14.3593\n",
      "10470 14.2134\n",
      "10480 11.4543\n",
      "10490 3.62842\n",
      "AVG accuracy 0.0\n",
      "10510 9.61662\n",
      "10520 9.97755\n",
      "10530 3.76552\n",
      "10540 2.46306\n",
      "AVG accuracy 0.0\n",
      "10560 7.12618\n",
      "10570 5.78158\n",
      "10580 3.48586\n",
      "10590 17.82\n",
      "AVG accuracy 0.08\n",
      "10610 11.9846\n",
      "Ignore step -1 -1\n",
      "10620 4.41877\n",
      "Ignore step 240 248\n",
      "10630 21.4801\n",
      "10640 14.9715\n",
      "AVG accuracy 0.0\n",
      "10660 22.008\n",
      "10670 4.14232\n",
      "10680 24.4936\n",
      "10690 12.442\n",
      "AVG accuracy 0.0\n",
      "10710 20.9165\n",
      "10720 5.96741\n",
      "10730 7.20902\n",
      "10740 7.53283\n",
      "AVG accuracy 0.0\n",
      "10760 0.180809\n",
      "10770 17.1586\n",
      "10780 3.20598\n",
      "10790 7.87702\n",
      "AVG accuracy 0.0\n",
      "10810 6.59556\n",
      "10820 4.62524\n",
      "10830 0.0\n",
      "Ignore step 448 452\n",
      "10840 7.76207\n",
      "AVG accuracy 0.0\n",
      "10860 11.8892\n",
      "10870 7.06264\n",
      "10880 6.80236\n",
      "10890 9.31786\n",
      "AVG accuracy 0.0\n",
      "10910 4.25168\n",
      "10920 9.09274\n",
      "10930 10.4994\n",
      "10940 15.1065\n",
      "AVG accuracy 0.0\n",
      "10960 10.115\n",
      "10970 8.00261\n",
      "10980 5.78704\n",
      "10990 1.48142\n",
      "AVG accuracy 0.0\n",
      "11010 5.20173\n",
      "11020 6.05028\n",
      "11030 6.36496\n",
      "11040 6.6496\n",
      "AVG accuracy 0.0\n",
      "11060 8.30866\n",
      "11070 4.77255\n",
      "11080 5.14116\n",
      "11090 4.61053\n",
      "AVG accuracy 0.0\n",
      "11110 6.10313\n",
      "11120 2.802\n",
      "11130 5.1283\n",
      "11140 6.14389\n",
      "AVG accuracy 0.0\n",
      "11160 6.07166\n",
      "11170 7.75215\n",
      "11180 5.93923\n",
      "11190 4.70407\n",
      "AVG accuracy 0.0\n",
      "11210 3.50866\n",
      "Ignore step -1 -1\n",
      "11220 5.51348\n",
      "11230 5.58119\n",
      "11240 3.83144\n",
      "AVG accuracy 0.08\n",
      "11260 0.0\n",
      "11270 7.7304\n",
      "11280 5.78508\n",
      "11290 3.56713\n",
      "AVG accuracy 0.08\n",
      "11310 14.6402\n",
      "11320 11.0275\n",
      "11330 11.1286\n",
      "Ignore step 524 525\n",
      "11340 24.9597\n",
      "AVG accuracy 0.1\n",
      "11360 12.5721\n",
      "11370 19.2339\n",
      "11380 22.9427\n",
      "11390 12.4873\n",
      "AVG accuracy 0.0\n",
      "11410 19.7057\n",
      "11420 11.0783\n",
      "11430 19.4593\n",
      "11440 5.84901\n",
      "AVG accuracy 0.0\n",
      "11460 18.5814\n",
      "11470 34.7707\n",
      "11480 69.776\n",
      "11490 6.74488\n",
      "AVG accuracy 0.0\n",
      "11510 15.9996\n",
      "11520 23.6683\n",
      "11530 12.5511\n",
      "11540 11.7389\n",
      "AVG accuracy 0.0\n",
      "11560 19.1947\n",
      "11570 26.6947\n",
      "11580 19.4595\n",
      "11590 13.4296\n",
      "AVG accuracy 0.0\n",
      "11610 10.5351\n",
      "11620 8.30111\n",
      "11630 29.2039\n",
      "11640 15.0131\n",
      "AVG accuracy 0.133333333333\n",
      "11660 5.83767\n",
      "11670 5.28477\n",
      "11680 8.09855\n",
      "11690 10.7227\n",
      "AVG accuracy 0.0\n",
      "11710 3.97524\n",
      "11720 12.3751\n",
      "11730 12.7\n",
      "11740 10.1235\n",
      "AVG accuracy 0.0\n",
      "11760 11.6194\n",
      "11770 5.51303\n",
      "11780 6.20745\n",
      "11790 5.91044\n",
      "AVG accuracy 0.0\n",
      "11810 19.0653\n",
      "11820 21.9329\n",
      "11830 12.4228\n",
      "11840 3.34214\n",
      "AVG accuracy 0.0\n",
      "11860 28.2887\n",
      "11870 11.2405\n",
      "11880 3.20464\n",
      "11890 8.62045\n",
      "AVG accuracy 0.0\n",
      "11910 13.4476\n",
      "11920 8.42908\n",
      "11930 5.00134\n",
      "11940 6.15003\n",
      "AVG accuracy 0.133333333333\n",
      "11960 7.17139\n",
      "11970 6.97763\n",
      "11980 6.35421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11990 7.94328\n",
      "AVG accuracy 0.0\n",
      "Ignore step -1 -1\n",
      "12020 4.81822\n",
      "12030 2.72811\n",
      "Ignore step -1 -1\n",
      "12040 13.8907\n",
      "AVG accuracy 0.0\n",
      "12060 16.5139\n",
      "12070 17.0239\n",
      "12080 3.6954\n",
      "12090 9.52889\n",
      "AVG accuracy 0.0\n",
      "12110 76.3543\n",
      "12120 41.7396\n",
      "12130 1.25951\n",
      "12140 10.9987\n",
      "AVG accuracy 0.0\n",
      "12160 41.7188\n",
      "12170 40.4346\n",
      "12180 25.2489\n",
      "12190 17.1289\n",
      "AVG accuracy 0.0\n",
      "12210 16.562\n",
      "12220 58.8435\n",
      "12230 4.54462\n",
      "12240 27.0921\n",
      "AVG accuracy 0.0\n",
      "12260 11.453\n",
      "12270 19.2324\n",
      "Ignore step 297 298\n",
      "12290 31.3825\n",
      "Ignore step -1 -1\n",
      "AVG accuracy 0.0\n",
      "12310 30.3873\n",
      "12320 37.8748\n",
      "12330 8.60791\n",
      "12340 10.2829\n",
      "AVG accuracy 0.0\n",
      "12360 4.95986\n",
      "12370 9.76353\n",
      "Ignore step -1 -1\n",
      "12380 13.7728\n",
      "12390 2.93909\n",
      "AVG accuracy 0.133333333333\n",
      "12410 6.45876\n",
      "12420 7.81711\n",
      "12430 17.0796\n",
      "12440 13.8867\n",
      "AVG accuracy 0.0\n",
      "12460 12.0541\n",
      "12470 8.94754\n",
      "12480 8.87305\n",
      "12490 3.00784\n",
      "AVG accuracy 0.0\n",
      "12510 5.32952\n",
      "12520 3.46167\n",
      "12530 6.17082\n",
      "12540 5.01262\n",
      "AVG accuracy 0.0\n",
      "12560 6.25153\n",
      "12570 0.0\n",
      "12580 5.65104\n",
      "12590 2.52298\n",
      "AVG accuracy 0.0\n",
      "12610 3.13523\n",
      "12620 8.67644\n",
      "12630 4.58187\n",
      "12640 8.23002\n",
      "AVG accuracy 0.0\n",
      "12660 15.7914\n",
      "12670 13.8424\n",
      "12680 0.0\n",
      "12690 9.30333\n",
      "AVG accuracy 0.0\n",
      "12710 17.1078\n",
      "12720 13.5263\n",
      "12730 14.6872\n",
      "12740 15.338\n",
      "AVG accuracy 0.0\n",
      "12760 20.0886\n",
      "12770 6.46534\n",
      "12780 25.505\n",
      "12790 22.155\n",
      "AVG accuracy 0.0666666666667\n",
      "12810 2.22884\n",
      "12820 47.9577\n",
      "12830 19.7366\n",
      "12840 30.0075\n",
      "AVG accuracy 0.0\n",
      "12860 33.8559\n",
      "12870 48.5048\n",
      "12880 39.1538\n",
      "12890 31.9565\n",
      "AVG accuracy 0.0\n",
      "12910 19.9491\n",
      "12920 24.6776\n",
      "12930 7.13798\n",
      "12940 6.58509\n",
      "AVG accuracy 0.0\n",
      "12960 6.48049\n",
      "12970 4.53896\n",
      "12980 8.06702\n",
      "12990 11.4752\n",
      "AVG accuracy 0.0\n",
      "13010 9.32092\n",
      "13020 25.4419\n",
      "13030 1.82622\n",
      "13040 9.36279\n",
      "Ignore step 215 220\n",
      "AVG accuracy 0.0\n",
      "13060 15.9622\n",
      "13070 17.4717\n",
      "13080 13.3818\n",
      "13090 13.0342\n",
      "AVG accuracy 0.0\n",
      "13110 7.89911\n",
      "13120 4.81578\n",
      "13130 6.42709\n",
      "13140 10.6105\n",
      "AVG accuracy 0.1\n",
      "13160 8.38327\n",
      "13170 5.69169\n",
      "13180 14.0958\n",
      "13190 15.4855\n",
      "AVG accuracy 0.2\n",
      "13210 3.81882\n",
      "13220 8.2601\n",
      "13230 7.40883\n",
      "13240 7.70504\n",
      "AVG accuracy 0.0\n",
      "13260 6.47948\n",
      "13270 17.761\n",
      "13280 11.4204\n",
      "13290 26.8122\n",
      "AVG accuracy 0.2\n",
      "13310 15.772\n",
      "Ignore step 337 340\n",
      "13320 14.7052\n",
      "13330 7.57589\n",
      "13340 6.74263\n",
      "Ignore step 257 263\n",
      "AVG accuracy 0.0\n",
      "13360 9.64478\n",
      "13370 2.43405\n",
      "13380 6.27606\n",
      "Ignore step -1 -1\n",
      "13390 4.64123\n",
      "AVG accuracy 0.0\n",
      "13410 4.88247\n",
      "13420 9.26782\n",
      "13430 8.02501\n",
      "13440 3.65373\n",
      "AVG accuracy 0.0\n",
      "13460 9.14336\n",
      "13470 5.04578\n",
      "13480 12.2352\n",
      "13490 6.01073\n",
      "AVG accuracy 0.1\n",
      "13510 10.8344\n",
      "13520 28.5087\n",
      "13530 20.1691\n",
      "13540 12.3311\n",
      "AVG accuracy 0.0\n",
      "13560 3.12525\n",
      "Ignore step 305 306\n",
      "13570 0.0\n",
      "13580 15.8542\n",
      "13590 24.9168\n",
      "AVG accuracy 0.0\n",
      "13610 0.0\n",
      "13620 27.9208\n",
      "13630 0.0\n",
      "13640 31.5228\n",
      "AVG accuracy 0.1\n",
      "13660 10.8513\n",
      "13670 8.01397\n",
      "13680 7.50406\n",
      "13690 8.0184\n",
      "AVG accuracy 0.0\n",
      "13710 2.80134\n",
      "13720 21.8621\n",
      "13730 23.5968\n",
      "13740 25.3003\n",
      "AVG accuracy 0.1\n",
      "13760 5.33253\n",
      "13770 19.7121\n",
      "13780 25.1935\n",
      "13790 19.1563\n",
      "AVG accuracy 0.0\n",
      "13810 22.7924\n",
      "13820 25.0815\n",
      "13830 47.4338\n",
      "13840 19.5017\n",
      "AVG accuracy 0.0\n",
      "13860 18.6082\n",
      "13870 6.72425\n",
      "13880 3.92755\n",
      "13890 27.0736\n",
      "AVG accuracy 0.0\n",
      "13910 29.8967\n",
      "13920 39.0348\n",
      "13930 41.8628\n",
      "13940 55.2352\n",
      "AVG accuracy 0.0\n",
      "13960 42.1857\n",
      "13970 13.0556\n",
      "13980 30.5804\n",
      "13990 48.9256\n",
      "AVG accuracy 0.0\n",
      "14010 8.90732\n",
      "14020 3.17037\n",
      "14030 31.8528\n",
      "14040 9.78475\n",
      "AVG accuracy 0.0\n",
      "14060 15.6039\n",
      "14070 12.3624\n",
      "14080 31.7724\n",
      "14090 77.4947\n",
      "AVG accuracy 0.0\n",
      "14110 7.03039\n",
      "14120 10.8063\n",
      "14130 9.08158\n",
      "14140 9.49119\n",
      "AVG accuracy 0.0\n",
      "14160 53.7859\n",
      "14170 0.0\n",
      "14180 60.752\n",
      "14190 79.8095\n",
      "AVG accuracy 0.0\n",
      "14210 14.4178\n",
      "14220 18.366\n",
      "14230 13.611\n",
      "14240 6.29255\n",
      "AVG accuracy 0.1\n",
      "14260 10.4191\n",
      "14270 9.38425\n",
      "14280 6.53628\n",
      "14290 8.70755\n",
      "AVG accuracy 0.0\n",
      "14310 28.0792\n",
      "14320 34.0118\n",
      "14330 31.8256\n",
      "14340 44.6757\n",
      "AVG accuracy 0.133333333333\n",
      "14360 21.7976\n",
      "14370 27.9135\n",
      "14380 30.9431\n",
      "14390 40.4636\n",
      "AVG accuracy 0.0\n",
      "14410 70.5301\n",
      "14420 76.054\n",
      "14430 71.7192\n",
      "14440 76.5526\n",
      "AVG accuracy 0.0\n",
      "14460 36.6916\n",
      "14470 31.5842\n",
      "14480 24.0976\n",
      "14490 13.1996\n",
      "AVG accuracy 0.0\n",
      "14510 4.46606\n",
      "14520 15.7437\n",
      "14530 12.7566\n",
      "14540 36.4793\n",
      "AVG accuracy 0.0\n",
      "14560 45.1458\n",
      "14570 55.8393\n",
      "14580 14.7884\n",
      "14590 27.1725\n",
      "AVG accuracy 0.1\n",
      "14610 13.7695\n",
      "14620 18.9016\n",
      "14630 8.45046\n",
      "14640 16.0648\n",
      "AVG accuracy 0.0\n",
      "14660 6.76838\n",
      "Ignore step -1 -1\n",
      "14670 3.21498\n",
      "14680 15.5975\n",
      "14690 0.0\n",
      "AVG accuracy 0.0\n",
      "14710 18.2819\n",
      "14720 7.54135\n",
      "14730 6.52358\n",
      "14740 8.64401\n",
      "AVG accuracy 0.0\n",
      "14760 18.401\n",
      "14770 35.1766\n",
      "14780 23.1752\n",
      "Ignore step -1 -1\n",
      "14790 56.8742\n",
      "AVG accuracy 0.08\n",
      "Ignore step -1 -1\n",
      "14810 63.9203\n",
      "14820 0.0\n",
      "14830 49.166\n",
      "Ignore step -1 -1\n",
      "14840 88.1795\n",
      "AVG accuracy 0.0\n",
      "14860 121.702\n",
      "14870 61.0294\n",
      "14880 51.8209\n",
      "14890 0.0\n",
      "AVG accuracy 0.0\n",
      "14910 71.1969\n",
      "14920 81.9372\n",
      "14930 65.911\n",
      "14940 48.6117\n",
      "AVG accuracy 0.0\n",
      "14960 195.175\n",
      "14970 31.2183\n",
      "14980 6.38013\n",
      "14990 2.03459\n",
      "AVG accuracy 0.1\n",
      "15010 52.9873\n",
      "15020 15.1794\n",
      "15030 17.3906\n",
      "15040 22.8316\n",
      "AVG accuracy 0.0\n",
      "15060 39.6261\n",
      "15070 37.9521\n",
      "15080 15.2838\n",
      "15090 31.4296\n",
      "AVG accuracy 0.1\n",
      "15110 77.6735\n",
      "15120 0.0\n",
      "15130 33.7158\n",
      "15140 22.4861\n",
      "AVG accuracy 0.0\n",
      "15160 3.47364\n",
      "15170 8.02698\n",
      "15180 8.00799\n",
      "15190 6.26886\n",
      "AVG accuracy 0.0\n",
      "15210 29.6981\n",
      "15220 11.3793\n",
      "15230 28.8525\n",
      "15240 66.7368\n",
      "AVG accuracy 0.0\n",
      "15260 93.6882\n",
      "15270 44.2113\n",
      "15280 46.1562\n",
      "15290 148.803\n",
      "AVG accuracy 0.0\n",
      "15310 55.7569\n",
      "15320 14.6137\n",
      "15330 8.66452\n",
      "15340 8.45989\n",
      "AVG accuracy 0.0\n",
      "15360 11.0897\n",
      "15370 6.60354\n",
      "15380 8.97717\n",
      "15390 6.08595\n",
      "AVG accuracy 0.157142857143\n",
      "15410 6.59301\n",
      "15420 15.6813\n",
      "15430 19.2106\n",
      "Ignore step -1 -1\n",
      "15440 15.6873\n",
      "AVG accuracy 0.0\n",
      "15460 5.06442\n",
      "15470 7.78505\n",
      "15480 11.6826\n",
      "15490 23.7481\n",
      "AVG accuracy 0.0\n",
      "15510 42.6941\n",
      "15520 37.9915\n",
      "15530 17.1415\n",
      "15540 29.9513\n",
      "AVG accuracy 0.0\n",
      "15560 26.9131\n",
      "Ignore step 438 439\n",
      "15570 30.2693\n",
      "15580 33.88\n",
      "15590 42.8419\n",
      "AVG accuracy 0.0\n",
      "15610 78.9393\n",
      "15620 85.257\n",
      "15630 49.8023\n",
      "15640 17.1367\n",
      "AVG accuracy 0.0\n",
      "15660 24.3772\n",
      "15670 59.255\n",
      "15680 0.0\n",
      "15690 57.4277\n",
      "AVG accuracy 0.0\n",
      "15710 114.109\n",
      "15720 98.3607\n",
      "15730 107.137\n",
      "15740 98.6078\n",
      "AVG accuracy 0.0\n",
      "15760 75.7541\n",
      "15770 45.7938\n",
      "15780 91.6791\n",
      "Ignore step 213 217\n",
      "15790 27.5252\n",
      "AVG accuracy 0.0\n",
      "15810 83.5273\n",
      "15820 56.7797\n",
      "15830 18.8663\n",
      "15840 96.8409\n",
      "AVG accuracy 0.0\n",
      "15860 138.547\n",
      "15870 126.771\n",
      "15880 62.445\n",
      "15890 22.7287\n",
      "AVG accuracy 0.0\n",
      "15910 83.677\n",
      "15920 16.6829\n",
      "15930 92.6937\n",
      "15940 103.623\n",
      "AVG accuracy 0.0\n",
      "15960 40.4466\n",
      "15970 55.6172\n",
      "15980 52.6872\n",
      "15990 2.51322\n",
      "End\n"
     ]
    }
   ],
   "source": [
    "#=========== Training ==================\n",
    "\n",
    "\n",
    "def accuracyValidation(acc_batch_size, step):\n",
    "    acc_accum = 0\n",
    "    for step_accuracy_ in range(acc_batch_size):\n",
    "        start_true, end_true, doc, que, doc_v, que_v = sess.run(next_element_valid)\n",
    "        acc, stat, s, e = sess.run(\n",
    "            (accuracy, summary_op, pr_start_idx, pr_end_idx),\n",
    "            feed_dict={question_ph: [que_v], document_ph: [doc_v], start_end_true: [start_true, end_true]}\n",
    "        )\n",
    "        #print('Predicted answer', utils.substr(doc, s, e))\n",
    "        #print('True answer', utils.substr(doc, start_true, end_true))\n",
    "        writer.add_summary(stat,  step* 10 + step_accuracy_)\n",
    "        acc_accum += acc;\n",
    "    print('AVG accuracy', acc_accum/acc_batch_size)\n",
    "\n",
    "def trainStep(step):\n",
    "    start_true, end_true, doc, que, doc_v, que_v = sess.run(next_element)\n",
    "    \n",
    "    if start_true < 0 or end_true > max_sequence_length - 1: \n",
    "        print('Ignore step', start_true, end_true)\n",
    "        return\n",
    "    \n",
    "    _,loss, stat = sess.run(\n",
    "        (train_step, sum_loss, summary_op), \n",
    "        feed_dict={question_ph: [que_v], document_ph: [doc_v], start_end_true: [start_true, end_true]}\n",
    "    )\n",
    "    if step % 10 == 0: print(step, loss)\n",
    "    writer.add_summary(stat,  step)\n",
    "\n",
    "\n",
    "dataset = ds.getDataset([\"./train_train_task_b.csv\"], max_sequence_length)\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "dataset_validation = ds.getDataset([\"./valid_train_task_b.csv\"], max_sequence_length)\n",
    "iterator_valid = dataset_validation.make_one_shot_iterator()\n",
    "next_element_valid = iterator_valid.get_next()\n",
    "\n",
    "summary_op = tf.summary.merge_all()\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    writer = tf.summary.FileWriter(FLAGS.log_path + \"/5\", sess.graph)\n",
    "    for epoch_ in range(max_epoch):\n",
    "        for step_ in range(16000):\n",
    "            if step_ % 50 == 0:\n",
    "                # --------- ACCURACY -------------\n",
    "                accuracyValidation(acc_batch_size, step_)\n",
    "  \n",
    "            else:\n",
    "                trainStep(step_)\n",
    "                \n",
    "    print('End')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
