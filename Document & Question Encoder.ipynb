{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "#import maxout\n",
    "import highway_maxout as hmn\n",
    "import utils\n",
    "import dataset as ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#======= FLAGS ==========\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "tf.app.flags.DEFINE_integer('maxout_layer_size', 8, 'Maxout layer size')\n",
    "tf.app.flags.DEFINE_integer('maxout_pooling_size', 16, 'Maxout pooling size')\n",
    "tf.app.flags.DEFINE_integer('lstm_size', 10, 'LSTM cell internal size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all variables\n",
    "tf.reset_default_graph();\n",
    "\n",
    "lstm_size = 10\n",
    "#question_size = 5\n",
    "#document_size = 8\n",
    "word_vector_size = 300\n",
    "maxout_pooling_size = 16\n",
    "max_decoder_iterations = 4\n",
    "maxout_layer_size = 8;\n",
    "max_epoch = 1;\n",
    "max_sequence_length = 11\n",
    "#training_set_size = 100;\n",
    "\n",
    "# \n",
    "question_ph = tf.placeholder(tf.float32, [1, max_sequence_length, word_vector_size])\n",
    "document_ph = tf.placeholder(tf.float32, [1, max_sequence_length, word_vector_size])\n",
    "\n",
    "# LSTM cell initialization\n",
    "lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "\n",
    "\n",
    "# LSTM cells for Bi-LSTM for COATINATION ENCODER\n",
    "lstm_cenc_fw = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "lstm_cenc_bw = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "\n",
    "# create lstm cell for DYNAMIC POINTING DECODER\n",
    "lstm_dec = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "# get lstm initial state of zeroes\n",
    "lstm_dec_state = lstm_dec.zero_state(1, tf.float32)\n",
    "start_pos = 0; # generate random between (0, document_size-1)\n",
    "end_pos = 0;   # generate random between (0, document_size-1)\n",
    "\n",
    "# create sentinel vector variable for both encodings \n",
    "#with tf.variable_scope(\"scope1\") as scope:\n",
    "sentinel_q = tf.get_variable(\"sentinel_q\", [ lstm_size , 1], initializer = tf.random_normal_initializer())\n",
    "sentinel_d = tf.get_variable(\"sentinel_d\", [ lstm_size , 1], initializer = tf.random_normal_initializer()) \n",
    "\n",
    "# optimizer\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"transpose_6:0\", dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# r = lstm(inputs = tf.convert_to_tensor([[1,2], [2,3]], dtype=tf.float32), state = zero_state_q)\n",
    "\n",
    "def length(sequence):\n",
    "  used = tf.sign(tf.reduce_max(tf.abs(sequence), 2))\n",
    "  length = tf.reduce_sum(used, 1)\n",
    "  length = tf.cast(length, tf.int32)\n",
    "  return length\n",
    "\n",
    "'''\n",
    "transform tensor of shape [1, question_size, word_vector_size] to list of tensors of shape [1, word_vector_size]\n",
    "of length question_size. first dimenstion is batch size = 1\n",
    "'''\n",
    "\n",
    "#print(tf.shape(question_ph)[1])\n",
    "question_input = tf.unstack(question_ph, max_sequence_length, 1)\n",
    "document_input = tf.unstack(document_ph, max_sequence_length, 1)\n",
    "#print(x)\n",
    "\n",
    "# we use the same LSTM for both encodings\n",
    "outputs_q, state_q = tf.contrib.rnn.static_rnn(lstm, inputs = question_input, sequence_length = length(question_ph), dtype=tf.float32)\n",
    "outputs_d, state_d = tf.contrib.rnn.static_rnn(lstm, inputs = document_input, sequence_length = length(document_ph), dtype=tf.float32)\n",
    "\n",
    "# \"squeeze\" transforms list of tensors of shape [1, lstm_size] of length L to tensor of shape [L, lstm_size]\n",
    "que_enc = tf.transpose(tf.squeeze(outputs_q))\n",
    "doc_enc = tf.transpose(tf.squeeze(outputs_d))\n",
    "document_size =tf.shape(doc_enc)[1]\n",
    "\n",
    "\n",
    "# append sentinel vector for both encodings \n",
    "doc_enc_sentinel = tf.concat([doc_enc, sentinel_d], axis = 1)\n",
    "que_enc_sentinel = utils.non_linear_projection(tf.concat([que_enc, sentinel_q], axis = 1))\n",
    "\n",
    "\n",
    "# ===================  COATTENTION ENCODER ===================\n",
    "# L \\in R(doc_size + 1) x (que_size + 1)\n",
    "L = tf.matmul(doc_enc_sentinel, que_enc_sentinel, transpose_a = True)\n",
    "A_Q = tf.nn.softmax(L, 0)\n",
    "A_D = tf.nn.softmax(tf.transpose(L), 1)\n",
    "C_Q = tf.matmul(doc_enc_sentinel, A_Q)\n",
    "# C_D \\in R_2*lstm_size x (doc_size + 1)\n",
    "C_D = tf.matmul(tf.concat([que_enc_sentinel, C_Q], axis = 0), A_D)\n",
    "\n",
    "\n",
    "bi_lstm_input = tf.unstack(tf.reshape(tf.transpose(tf.concat([doc_enc_sentinel, C_D], axis = 0)), [max_sequence_length + 1, 1, 3*lstm_size]))\n",
    "outputs_bi, output_state_fw, output_state_bw = tf.nn.static_bidirectional_rnn(lstm_cenc_fw, lstm_cenc_bw, inputs = bi_lstm_input , dtype=tf.float32)\n",
    "U = tf.transpose(tf.squeeze(tf.slice(outputs_bi, [0,0,0], [document_size, 1,  2*lstm_size])))\n",
    "#U.set_shape([2*lstm_size, document_size])\n",
    "print(U)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== DYNAMIC POINTING DECODER =============\n",
    "\n",
    "\n",
    "#scope = tf.get_variable_scope()\n",
    "#u_t = get_scope_variable(scope, 'hmn_u_t', [2*lstm_size, 1]) \n",
    "#h_i = get_scope_variable(scope, 'hmn_h_i', [lstm_size, 1 ]) \n",
    "#u_s_i = get_scope_variable(scope, 'hmn_u_s_i', [2*lstm_size, 1])\n",
    "#u_e_i = get_scope_variable(scope, 'hmn_u_e_i', [2*lstm_size, 1])\n",
    "\n",
    "\n",
    "#m_3 = HMN(U, h_i, u_s_i, u_e_i)\n",
    "#print(m_3)\n",
    "\n",
    "# returns tuple (scores_start, scores_end, strart_pos, start_end, new_lstm_state)\n",
    "def decoderIteration(U, lstm_state, start_pos, end_pos):\n",
    "    #print(tf.slice(U, [0,0], [lstm_size*2, 0]))\n",
    "    #print(tf.slice(U, [0, start_pos], [lstm_size*2, 1], name='slice-1x'))\n",
    "    \n",
    "    scores_start = hmn.HMN(U, \n",
    "                       tf.transpose(lstm_state.h), \n",
    "                       tf.slice(U, [0, start_pos], [lstm_size*2, 1]) ,\n",
    "                       tf.slice(U, [0, end_pos], [lstm_size*2, 1],) , \n",
    "                       'start',\n",
    "                        FLAGS)\n",
    "\n",
    "    new_start_pos = tf.to_int32(tf.argmax(scores_start, 0))\n",
    "\n",
    "    #print(lstm_state)\n",
    "    scores_end = hmn.HMN(U, \n",
    "                     tf.transpose(lstm_state.h), \n",
    "                     tf.slice(U, [0, new_start_pos], [lstm_size*2, 1],) ,\n",
    "                     tf.slice(U, [0, end_pos], [lstm_size*2, 1],), \n",
    "                    'end',\n",
    "                    FLAGS)\n",
    "    new_end_pos = tf.to_int32(tf.argmax(scores_end, 0))\n",
    "    lstm_input = tf.concat(\n",
    "        [tf.slice(U, [0, new_start_pos], [lstm_size*2, 1], name='slice-5'), tf.slice(U, [0, new_end_pos], [lstm_size*2, 1])],\n",
    "        axis = 0\n",
    "    )\n",
    "    output, new_lstm_state = lstm_dec(tf.reshape(lstm_input, [1, lstm_size*4]), lstm_state)\n",
    "    #print(new_lstm_state)\n",
    "    return scores_start, scores_end, new_start_pos , new_end_pos, new_lstm_state\n",
    "\n",
    "\n",
    "start_pos = 0;\n",
    "end_pos = 0;\n",
    "sum_start_scores = tf.zeros([1, document_size])\n",
    "sum_end_scores = tf.zeros([1, document_size])\n",
    "lstm_dec_state = lstm_dec.zero_state(1, tf.float32)\n",
    "#print(lstm_dec_state)\n",
    "\n",
    "for step in range(max_decoder_iterations):\n",
    "    scores_start, scores_end, new_start_pos, new_end_pos, lstm_dec_state = decoderIteration(U, lstm_dec_state, start_pos, end_pos)\n",
    "    sum_start_scores = tf.add(sum_start_scores, scores_start)\n",
    "    sum_end_scores   = tf.add(sum_end_scores, scores_end)\n",
    "    if new_start_pos == start_pos and end_pos == new_end_pos : break\n",
    "    start_pos = new_start_pos\n",
    "    end_pos = new_end_pos\n",
    "\n",
    "    \n",
    "# loss and train step\n",
    "start_end_true = tf.placeholder(tf.int32, [2]);\n",
    "#end_true = tf.placeholder(tf.int32, ());\n",
    "onehot_labels = tf.one_hot(start_end_true, document_size)\n",
    "sum_loss = tf.losses.softmax_cross_entropy(\n",
    "    onehot_labels,\n",
    "    tf.concat([sum_start_scores, sum_end_scores], axis=0))\n",
    "\n",
    "#print(sum_loss)\n",
    "\n",
    "\n",
    "train_step = optimizer.minimize(sum_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "24\n",
      "0.0\n",
      "43\n",
      "45\n",
      "0.0\n",
      "107\n",
      "110\n",
      "0.0\n",
      "err i\n",
      "err u\n",
      "106\n",
      "108\n",
      "0.0\n",
      "73\n",
      "75\n",
      "0.0\n",
      "End\n"
     ]
    }
   ],
   "source": [
    "#=========== Training ==================\n",
    "dataset = ds.getDataset([\"./train_train_task_b.csv\"], max_sequence_length)\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch_ in range(max_epoch):\n",
    "        for step_ in range(5):\n",
    "            n = sess.run(next_element)\n",
    "            print(n[0])\n",
    "            print(n[1])\n",
    "            \n",
    "            sess.run(train_step, feed_dict={question_ph: [n[5]], document_ph: [n[4]], start_end_true: [n[0], n[1]]})\n",
    "            print(sess.run(sum_loss, feed_dict={question_ph: [n[5]], document_ph: [n[4]], start_end_true: [n[0], n[1]]}))\n",
    "    print('End')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Reshape_550:0\", shape=(2,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.nn.softmax(tf.convert_to_tensor([1,2], dtype=tf.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ff(a):\n",
    "    def _ff(b):\n",
    "        return a + b\n",
    "    return _ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ff(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function ff.<locals>._ff at 0x7fee89f202f0>\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
