{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "#import maxout\n",
    "import highway_maxout as hmn\n",
    "import utils\n",
    "import dataset as ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#======= FLAGS ==========\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "tf.app.flags.DEFINE_integer('maxout_layer_size', 8, 'Maxout layer size')\n",
    "tf.app.flags.DEFINE_integer('maxout_pooling_size', 16, 'Maxout pooling size')\n",
    "tf.app.flags.DEFINE_integer('lstm_size', 200, 'LSTM cell internal size')\n",
    "tf.app.flags.DEFINE_string('log_path', '/tmp/dcn', 'logs location')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all variables\n",
    "tf.reset_default_graph();\n",
    "\n",
    "lstm_size = 200\n",
    "word_vector_size = 300\n",
    "maxout_pooling_size = 16\n",
    "max_decoder_iterations = 4\n",
    "maxout_layer_size = 8;\n",
    "max_epoch = 1;\n",
    "max_sequence_length = 600\n",
    "#training_set_size = 100;\n",
    "\n",
    "# \n",
    "question_ph = tf.placeholder(tf.float32, [1, max_sequence_length, word_vector_size], name=\"q_input\")\n",
    "document_ph = tf.placeholder(tf.float32, [1, max_sequence_length, word_vector_size], name=\"d_input\")\n",
    "\n",
    "\n",
    "with tf.name_scope('ENCODER'):\n",
    "    # LSTM cell initialization\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "\n",
    "\n",
    "# LSTM cells for Bi-LSTM for COATINATION ENCODER\n",
    "with tf.name_scope('COATTENTION_ENCODER'):\n",
    "    lstm_cenc_fw = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    lstm_cenc_bw = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "\n",
    "# create lstm cell for DYNAMIC POINTING DECODER\n",
    "lstm_dec = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "# get lstm initial state of zeroes\n",
    "#lstm_dec_state = lstm_dec.zero_state(1, tf.float32)\n",
    "start_pos = 0; # generate random between (0, document_size-1)\n",
    "end_pos = 0;   # generate random between (0, document_size-1)\n",
    "\n",
    "# create sentinel vector variable for both encodings \n",
    "#with tf.variable_scope(\"scope1\") as scope:\n",
    "sentinel_q = tf.get_variable(\"sentinel_q\", [ lstm_size , 1], initializer = tf.random_normal_initializer())\n",
    "sentinel_d = tf.get_variable(\"sentinel_d\", [ lstm_size , 1], initializer = tf.random_normal_initializer()) \n",
    "\n",
    "# optimizer\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"COATTENTION_ENCODER_1/strided_slice:0\", shape=(601, 400), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# r = lstm(inputs = tf.convert_to_tensor([[1,2], [2,3]], dtype=tf.float32), state = zero_state_q)\n",
    "\n",
    "def length(sequence):\n",
    "  used = tf.sign(tf.reduce_max(tf.abs(sequence), 2))\n",
    "  length = tf.reduce_sum(used, 1)\n",
    "  length = tf.cast(length, tf.int32)\n",
    "  return length\n",
    "\n",
    "'''\n",
    "transform tensor of shape [1, question_size, word_vector_size] to list of tensors of shape [1, word_vector_size]\n",
    "of length question_size. first dimenstion is batch size = 1\n",
    "'''\n",
    "\n",
    "#print(tf.shape(question_ph)[1])\n",
    "#question_input = tf.unstack(question_ph, max_sequence_length, 1)\n",
    "#document_input = tf.unstack(document_ph, max_sequence_length, 1)\n",
    "#print(x)\n",
    "\n",
    "# we use the same LSTM for both encodings\n",
    "#outputs_q, state_q = tf.contrib.rnn.static_rnn(lstm, inputs = question_input, sequence_length = length(question_ph), dtype=tf.float32)\n",
    "#outputs_d, state_d = tf.contrib.rnn.static_rnn(lstm, inputs = document_input, sequence_length = length(document_ph), dtype=tf.float32)\n",
    "\n",
    "with tf.name_scope('ENCODER'):\n",
    "    outputs_q, state_q = tf.nn.dynamic_rnn(lstm, inputs = question_ph, sequence_length = length(question_ph), dtype=tf.float32)\n",
    "    outputs_d, state_d = tf.nn.dynamic_rnn(lstm, inputs = document_ph, sequence_length = length(document_ph), dtype=tf.float32)\n",
    "\n",
    "\n",
    "# \"squeeze\" transforms list of tensors of shape [1, lstm_size] of length L to tensor of shape [L, lstm_size]\n",
    "que_enc = tf.transpose(tf.squeeze(outputs_q))\n",
    "doc_enc = tf.transpose(tf.squeeze(outputs_d))\n",
    "document_size =tf.shape(doc_enc)[1]\n",
    "\n",
    "\n",
    "# append sentinel vector for both encodings \n",
    "doc_enc_sentinel = tf.concat([doc_enc, sentinel_d], axis = 1)\n",
    "que_enc_sentinel = utils.non_linear_projection(tf.concat([que_enc, sentinel_q], axis = 1))\n",
    "\n",
    "\n",
    "# ===================  COATTENTION ENCODER ===================\n",
    "with tf.name_scope('COATTENTION_ENCODER'):\n",
    "    # L \\in R(doc_size + 1) x (que_size + 1)\n",
    "    L = tf.matmul(doc_enc_sentinel, que_enc_sentinel, transpose_a = True)\n",
    "    A_Q = tf.nn.softmax(L, 0)\n",
    "    A_D = tf.nn.softmax(tf.transpose(L), 1)\n",
    "    C_Q = tf.matmul(doc_enc_sentinel, A_Q)\n",
    "    # C_D \\in R_2*lstm_size x (doc_size + 1)\n",
    "    C_D = tf.matmul(tf.concat([que_enc_sentinel, C_Q], axis = 0), A_D)\n",
    "\n",
    "    # bi_lstm_input = tf.unstack(tf.reshape(tf.transpose(tf.concat([doc_enc_sentinel, C_D], axis = 0)), [max_sequence_length + 1, 1, 3*lstm_size]))\n",
    "    # TODO Q: would we use single cell of two different\n",
    "    bi_lstm_input = tf.concat([doc_enc_sentinel, C_D], axis = 0)\n",
    "    bi_lstm_input = tf.transpose(bi_lstm_input)\n",
    "    bi_lstm_input = tf.reshape(bi_lstm_input, [1, max_sequence_length + 1, 3*lstm_size])\n",
    "    test = tf.placeholder(tf.float32, [1,12,30]);\n",
    "    outputs_bi, output_state = tf.nn.bidirectional_dynamic_rnn(\n",
    "        cell_fw = lstm_cenc_fw, \n",
    "        cell_bw = lstm_cenc_fw,\n",
    "      #  cell_bw = lstm_cenc_bw,\n",
    "        inputs = bi_lstm_input,\n",
    "      #  sequence_length = [12],\n",
    "        dtype=tf.float32\n",
    "    )\n",
    "\n",
    "    # we take first because of we feed to bi-RNN only one sentence\n",
    "    outputs_bi = tf.concat(outputs_bi, axis=2)[0]\n",
    "    print(outputs_bi)\n",
    "    U = tf.slice(outputs_bi, [0,0], [document_size, 2*lstm_size])\n",
    "    U = tf.transpose(U)\n",
    "#print(U)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== DYNAMIC POINTING DECODER =============\n",
    "\n",
    "\n",
    "#scope = tf.get_variable_scope()\n",
    "#u_t = get_scope_variable(scope, 'hmn_u_t', [2*lstm_size, 1]) \n",
    "#h_i = get_scope_variable(scope, 'hmn_h_i', [lstm_size, 1 ]) \n",
    "#u_s_i = get_scope_variable(scope, 'hmn_u_s_i', [2*lstm_size, 1])\n",
    "#u_e_i = get_scope_variable(scope, 'hmn_u_e_i', [2*lstm_size, 1])\n",
    "\n",
    "\n",
    "#m_3 = HMN(U, h_i, u_s_i, u_e_i)\n",
    "#print(m_3)\n",
    "\n",
    "# returns tuple (scores_start, scores_end, strart_pos, start_end, new_lstm_state)\n",
    "def decoderIteration(U, lstm_state, start_pos, end_pos):\n",
    "    with tf.name_scope('Decoder_Iteration'):\n",
    "        with tf.name_scope('Next_Start'):\n",
    "            scores_start = hmn.HMN(U, \n",
    "                               tf.transpose(lstm_state.h), \n",
    "                               tf.slice(U, [0, start_pos], [lstm_size*2, 1]) ,\n",
    "                               tf.slice(U, [0, end_pos], [lstm_size*2, 1],) , \n",
    "                               'start',\n",
    "                                FLAGS)\n",
    "\n",
    "            new_start_pos = tf.to_int32(tf.argmax(scores_start, 0))\n",
    "\n",
    "        #print(lstm_state)\n",
    "        with tf.name_scope('Next_End'):\n",
    "            scores_end = hmn.HMN(U, \n",
    "                             tf.transpose(lstm_state.h), \n",
    "                             tf.slice(U, [0, new_start_pos], [lstm_size*2, 1],) ,\n",
    "                             tf.slice(U, [0, end_pos], [lstm_size*2, 1],), \n",
    "                            'end',\n",
    "                            FLAGS)\n",
    "            new_end_pos = tf.to_int32(tf.argmax(scores_end, 0))\n",
    "        \n",
    "        with tf.name_scope('LSTM_State_Update'):\n",
    "            lstm_input = tf.concat(\n",
    "                [tf.slice(U, [0, new_start_pos], [lstm_size*2, 1], name='slice-5'), tf.slice(U, [0, new_end_pos], [lstm_size*2, 1])],\n",
    "                axis = 0\n",
    "            )\n",
    "            output, new_lstm_state = lstm_dec(tf.reshape(lstm_input, [1, lstm_size*4]), lstm_state)\n",
    "        \n",
    "        #print(new_lstm_state)\n",
    "        return scores_start, scores_end, new_start_pos , new_end_pos, new_lstm_state\n",
    "\n",
    "\n",
    "\n",
    "#print(lstm_dec_state)\n",
    "\n",
    "with tf.name_scope('DYNAMIC_POINTING_DECODER'):\n",
    "    \n",
    "    start_pos = 0;\n",
    "    end_pos = 0;\n",
    "    sum_start_scores = tf.zeros([1, document_size])\n",
    "    sum_end_scores = tf.zeros([1, document_size])\n",
    "    lstm_dec_state = lstm_dec.zero_state(1, tf.float32)\n",
    "    \n",
    "    for step in range(max_decoder_iterations):\n",
    "        scores_start, scores_end, new_start_pos, new_end_pos, lstm_dec_state = decoderIteration(U, lstm_dec_state, start_pos, end_pos)\n",
    "        sum_start_scores = tf.add(sum_start_scores, scores_start)\n",
    "        sum_end_scores   = tf.add(sum_end_scores, scores_end)\n",
    "        if new_start_pos == start_pos and end_pos == new_end_pos : break\n",
    "        start_pos = new_start_pos\n",
    "        end_pos = new_end_pos\n",
    "\n",
    "    \n",
    "# loss and train step\n",
    "start_end_true = tf.placeholder(tf.int32, [2]);\n",
    "#end_true = tf.placeholder(tf.int32, ());\n",
    "onehot_labels = tf.one_hot(start_end_true, document_size)\n",
    "with tf.name_scope('Loss'):\n",
    "    sum_loss = tf.losses.softmax_cross_entropy(\n",
    "        onehot_labels,\n",
    "        tf.concat([sum_start_scores, sum_end_scores], axis=0))\n",
    "\n",
    "\n",
    "tf.summary.scalar('loss', sum_loss)\n",
    "with tf.name_scope('Train'):\n",
    "    train_step = optimizer.minimize(sum_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 6.45938\n",
      "1 6.38718\n",
      "2 6.59994\n",
      "3 7.34657\n",
      "4 7.27166\n",
      "5 6.96637\n",
      "6 6.52183\n",
      "7 6.85479\n",
      "8 6.62029\n",
      "9 5.29043\n",
      "10 7.0155\n",
      "11 5.7589\n",
      "12 7.79244\n",
      "13 6.26254\n",
      "14 7.3376\n",
      "15 7.66651\n",
      "16 6.3598\n",
      "17 5.33968\n",
      "18 6.86679\n",
      "19 6.44198\n",
      "20 6.55311\n",
      "21 5.34536\n",
      "22 6.48824\n",
      "23 5.26756\n",
      "24 5.40209\n",
      "25 7.12319\n",
      "26 6.89291\n",
      "27 6.10444\n",
      "28 5.41035\n",
      "29 5.40294\n",
      "30 5.23661\n",
      "31 6.36622\n",
      "32 8.71191\n",
      "33 4.17728\n",
      "34 5.47509\n",
      "35 8.99654\n",
      "36 6.79967\n",
      "37 4.50886\n",
      "38 7.04263\n",
      "39 6.21332\n",
      "40 4.39496\n",
      "41 8.79247\n",
      "42 8.62935\n",
      "43 5.60814\n",
      "44 5.71591\n",
      "45 6.6762\n",
      "46 7.29791\n",
      "47 5.28108\n",
      "48 5.7097\n",
      "49 6.21832\n",
      "50 8.39337\n",
      "51 8.1439\n",
      "52 4.81459\n",
      "53 6.68883\n",
      "54 5.69541\n",
      "55 3.8126\n",
      "56 4.53181\n",
      "57 6.72162\n",
      "58 4.8155\n",
      "59 5.68192\n",
      "60 5.88921\n",
      "61 2.67836\n",
      "62 6.56872\n",
      "63 11.6632\n",
      "64 9.56131\n",
      "65 6.63882\n",
      "66 7.23728\n",
      "67 6.21503\n",
      "68 8.0196\n",
      "69 6.62954\n",
      "70 7.9546\n",
      "71 6.45439\n",
      "72 6.14967\n",
      "73 6.3173\n",
      "74 6.22205\n",
      "75 5.71898\n",
      "76 5.17362\n",
      "77 6.1592\n",
      "78 5.02904\n",
      "79 7.06811\n",
      "80 6.73253\n",
      "81 4.13729\n",
      "82 5.46089\n",
      "83 5.39036\n",
      "84 4.20929\n",
      "85 4.64532\n",
      "86 6.67319\n",
      "87 3.63488\n",
      "88 4.74851\n",
      "89 5.80312\n",
      "90 4.01924\n",
      "91 6.74559\n",
      "92 5.82098\n",
      "93 5.74869\n",
      "94 5.71252\n",
      "95 5.76223\n",
      "96 3.96343\n",
      "97 4.70282\n",
      "98 4.24537\n",
      "99 3.61285\n",
      "100 4.69538\n",
      "101 5.48189\n",
      "102 6.46637\n",
      "103 7.17299\n",
      "104 3.84294\n",
      "105 4.62836\n",
      "106 3.53733\n",
      "107 4.50957\n",
      "108 4.02973\n",
      "109 6.16788\n",
      "110 5.40253\n",
      "111 11.8276\n",
      "112 5.96109\n",
      "113 6.95735\n",
      "114 7.12576\n",
      "115 6.31734\n",
      "116 6.31511\n",
      "117 7.48587\n",
      "118 6.90267\n",
      "119 5.0973\n",
      "120 5.2312\n",
      "121 5.5738\n",
      "122 7.5368\n",
      "123 5.77038\n",
      "124 6.99633\n",
      "125 3.36518\n",
      "126 4.61629\n",
      "127 8.05874\n",
      "128 6.57355\n",
      "129 7.45062\n",
      "130 4.33688\n",
      "131 6.43072\n",
      "132 6.11343\n",
      "133 7.48457\n",
      "134 5.85294\n",
      "135 5.01114\n",
      "136 6.5354\n",
      "137 5.99693\n",
      "138 5.22254\n",
      "139 4.68889\n",
      "140 4.94732\n",
      "141 5.40446\n",
      "142 5.79482\n",
      "143 4.68516\n",
      "144 6.8002\n",
      "145 5.59621\n",
      "146 3.98518\n",
      "147 5.43446\n",
      "148 3.62663\n",
      "149 7.57832\n",
      "150 6.99336\n",
      "151 4.89398\n",
      "152 4.76445\n",
      "153 5.91632\n",
      "154 6.14611\n",
      "155 5.80108\n",
      "156 5.52831\n",
      "157 4.62663\n",
      "158 4.342\n",
      "159 5.09683\n",
      "160 6.40867\n",
      "161 4.79219\n",
      "162 6.35064\n",
      "163 5.19566\n",
      "164 7.74155\n",
      "165 3.85212\n",
      "166 6.8861\n",
      "167 6.11912\n",
      "168 4.62779\n",
      "169 4.68821\n",
      "170 6.87868\n",
      "171 5.00575\n",
      "172 4.32403\n",
      "173 5.19527\n",
      "174 4.49779\n",
      "175 4.96121\n",
      "176 4.85982\n",
      "177 3.93437\n",
      "178 5.62004\n",
      "179 5.64985\n",
      "180 5.13839\n",
      "181 3.63651\n",
      "182 4.10896\n",
      "183 4.75303\n",
      "2509 4.79496\n",
      "2510 4.47875\n",
      "4854 4.36115\n",
      "4855 3.34603\n",
      "4856 4.40022\n"
     ]
    }
   ],
   "source": [
    "#=========== Training ==================\n",
    "\n",
    "dataset = ds.getDataset([\"./train_train_task_b.csv\"], max_sequence_length)\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "summary_op = tf.summary.merge_all()\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    writer = tf.summary.FileWriter(FLAGS.log_path, sess.graph)\n",
    "    for epoch_ in range(max_epoch):\n",
    "        for step_ in range(7000):\n",
    "            start_true, end_true, doc, que, doc_v, que_v = sess.run(next_element)\n",
    "            \n",
    "            if step_ % 100 == 99:\n",
    "                run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "                run_metadata = tf.RunMetadata()\n",
    "                _,loss,stat = sess.run(\n",
    "                    (train_step, sum_loss, summary_op), \n",
    "                    feed_dict={question_ph: [que_v], document_ph: [doc_v], start_end_true: [start_true, end_true]},\n",
    "                    options=run_options,\n",
    "                    run_metadata=run_metadata\n",
    "                )\n",
    "                print(step_, loss)\n",
    "                writer.add_run_metadata(run_metadata, 'step%d' % step_)\n",
    "                writer.add_summary(stat,  step_)\n",
    "            else: \n",
    "                _,loss, stat = sess.run(\n",
    "                    (train_step, sum_loss, summary_op), \n",
    "                    feed_dict={question_ph: [que_v], document_ph: [doc_v], start_end_true: [start_true, end_true]}\n",
    "                )\n",
    "                print(step_, loss)\n",
    "                writer.add_summary(stat,  step_)\n",
    "                \n",
    "    print('End')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Reshape_550:0\", shape=(2,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.nn.softmax(tf.convert_to_tensor([1,2], dtype=tf.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ff(a):\n",
    "    def _ff(b):\n",
    "        return a + b\n",
    "    return _ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = ff(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function ff.<locals>._ff at 0x7fee89f202f0>\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
